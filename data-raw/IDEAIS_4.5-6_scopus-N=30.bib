
@book{hamzei_place_2020,
	series = {Lecture {Notes} in {Geoinformation} and {Cartography}},
	title = {Place questions and human-generated answers: {A} data analysis approach},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85065802197&doi=10.1007%2f978-3-030-14745-7_1&partnerID=40&md5=4082624257bc09b1a50642322d8df7a5},
	abstract = {This paper investigates place-related questions submitted to search systems and their human-generated answers. Place-based search is motivated by the need to identify places matching some criteria, to identify them in space or relative to other places, or to characterize the qualities of such places. Human place-related questions have thus far been insufficiently studied and differ strongly from typical keyword queries. They thus challenge today’s search engines providing only rudimentary geographic information retrieval support. We undertake an analysis of the patterns in place-based questions using a large-scale dataset of questions/answers, MS MARCO V2.1. The results of this study reveal patterns that can inform the design of conversational search systems and in-situ assistance systems, such as autonomous vehicles. © Springer Nature Switzerland AG 2020.},
	author = {Hamzei, E. and Li, H. and Vasardani, M. and Baldwin, T. and Winter, S. and Tomko, M.},
	year = {2020},
	keywords = {Geographic information retrieval, Geographic questions, Query classification, Question answering systems, Web search queries},
	annote = {Export Date: 18 September 2019},
	notes = {041.pdf}
}

@article{subramani_robust_2019,
	title = {A robust artificial intelligence: smart indoor positioning system},
	volume = {8},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85070938359&doi=10.35940%2fijitee.A1001.0881019&partnerID=40&md5=5f978d760756155de7d2e74bdc10d166},
	doi = {10.35940/ijitee.A1001.0881019},
	abstract = {Over the previous few centuries, technology has converted massively from being a desktop personal computer to handheld mobile phones, with lower energy consumption of raw computing power. This computability is now incorporated with other systems as well as isolated to a single device. This paradigm was first noted in cyber-physical systems with the introduction of cloud services. The evolution of Artificial Intelligence(AI) with cloud computing and the importance of this field in human life, induce us to make simple and efficient talkative assistant robot for indoor navigation. The navigation system in outdoor typically rely upon Global Positioning System (GPS) but the indoor navigation systems have to rely on different technologies, as GPS signals cannot be received indoors. Thus, several technologies have been proposed and implemented over the past decade to improve navigation in indoors. But they were costly and less effective. Therefore, we have proposed a system that assists humans to find their location in a conversational manner. The suggested system was constructed by introducing the advantages of a personal assistant device, Amazon Alexa, the cloud services of Amazon and its voice services for indoor navigation. A Raspberry Pi 3 Model B is used as the element of the hardware to provide our system with intelligent characteristics. You can trigger the speech service using the "Alexa" keyword. Using the voice command, the skill/ application we created can be initiated. It operates a script on the cloud once Alexa is enabled, which runs a subroutine on the Raspberry Pi 3 in-turn to provide a path for that specific place. Once the Raspberry Pi calculation is finished, it sends the message back to Alexa. Alexa transforms the text into a voice and informs the user path. © BEIESP.},
	number = {10},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	author = {Subramani, B. and Deepika, S.P.},
	year = {2019},
	keywords = {Artificial Intelligence, Global Positioning System, Raspberry Pi, Voice command},
	pages = {1393--1398},
	annote = {Export Date: 18 September 2019},
	notes = {004.pdf}
}

@inproceedings{ali_design_2019,
	title = {Design of seamless multi-modal interaction framework for intelligent virtual agents in wearable mixed reality environment},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069173185&doi=10.1145%2f3328756.3328758&partnerID=40&md5=29caf4599720e0058100ba9c42524850},
	doi = {10.1145/3328756.3328758},
	abstract = {In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping. © 2019 Association for Computing Machinery.},
	author = {Ali, G. and Le, H.-Q. and Kim, J. and Hwang, S.-W. and Hwang, J.-I.},
	year = {2019},
	pages = {47--52},
	annote = {Export Date: 18 September 2019},
	notes = {021.pdf}
}

@inproceedings{acer_hyper-local_2019,
	title = {On hyper-local conversational agents in urban settings},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069164769&doi=10.1145%2f3325426.3329949&partnerID=40&md5=fc33decfa385e57d7205ee28f725197c},
	doi = {10.1145/3325426.3329949},
	abstract = {Conversational agents are increasingly becoming digital partners in our everyday computational experiences. Although rich, and fresh in content, these agents are completely oblivious to users' locality beyond geospatial weather and traffic conditions. In this position statement, we envisage a brand-new class of conversational agents that are hyper-local, embedded deeply in a local neighbourhood, e.g., at urban landmarks - providing rich, purposeful, detail, and in some cases playful information relevant to a neighbourhood. By design, these agents are spatially constrained, and one can only interact with them once she is in close vicinity at street-level granularity. Learning from quantitative (n = 1992) and qualitative (n = 21) studies, we identify a set of information that these agents must accommodate. Finally, we discuss the technical architecture of this class of conversational agents that leverages covert communication channel, edge AI and on-body devices for offering such hyper-local information access. © 2019 Copyright held by the owner/author(s).},
	author = {Acer, U.G. and Van Den Broeck, M. and Kawsar, F.},
	year = {2019},
	keywords = {Citizen Engagement, Conversational Agent, Edge AI, Spontaneous Interaction},
	pages = {1--4},
	annote = {Export Date: 18 September 2019},
	notes = {038.pdf}
}

@book{sardella_approach_2019,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {An {Approach} to {Conversational} {Recommendation} of {Restaurants}},
	volume = {1034},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069684027&doi=10.1007%2f978-3-030-23525-3_16&partnerID=40&md5=c85ed4154f736bc502cab11cfbd57c23},
	abstract = {In this paper, we propose an approach based on the integration of a chatbot module, a location-based service, and a recommendation algorithm. This approach has been deployed for restaurant recommendation, tested on a sample of 50 real users, and compared with some state-of-the-art algorithms. The preliminary experimental results showed the benefits of the proposed approach in terms of performance. An ANOVA test enabled us to verify the statistical significance of the obtained findings. © Springer Nature Switzerland AG 2019.},
	author = {Sardella, N. and Biancalana, C. and Micarelli, A. and Sansonetti, G.},
	year = {2019},
	keywords = {Cold-start, Conversational recommender systems, Location-based services},
	annote = {Export Date: 18 September 2019},
	notes = {007.pdf}
}

@inproceedings{anelli_anna:_2019,
	title = {Anna: {A} virtual assistant to interact with puglia digital library (discussion paper)},
	volume = {2400},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069495101&partnerID=40&md5=2d71c59263f406106d92dea993a6d07b},
	abstract = {In the last years, a huge amount of data has been released by private and public bodies as Linked Open Data. By their inner nature, these data contain rich semantic information that can be automatically processed by software agents and explored by humans via visual tools or structured SPARQL queries. Although they result useful for many tasks, these latter approaches miss the simplicity of the interfaces based on interactions via natural language implemented in modern virtual assistants. In this paper, we present a system able to assist the user in exploring the knowledge exposed by the Puglia Digital Library containing information and data associated with digital goods related to the Apulia region in Italy. We show how to interact with the Digital Library by means of a virtual assistant and how, thanks to its publication as Linked Open Data, it is possible to easily integrate it on-the-fly with external knowledge sources such as geographical ones. Copyright © 2019 for the individual papers by the papers authors.},
	author = {Anelli, V.W. and Di Noia, T. and Di Sciascio, E. and Ragone, A.},
	year = {2019},
	keywords = {Chatbot, Digital Libraries, Linked Open Data, Vocal Assistant},
	annote = {Export Date: 18 September 2019},
	notes = {008.pdf}
}

@article{massai_paval:_2019,
	title = {{PAVAL}: {A} location-aware virtual personal assistant for retrieving geolocated points of interest and location-based services},
	volume = {77},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85054452779&doi=10.1016%2fj.engappai.2018.09.013&partnerID=40&md5=e0cbc5c9993e1edf098305b631f45fb7},
	doi = {10.1016/j.engappai.2018.09.013},
	abstract = {Today most of the users on the move require contextualized local and georeferenced information. Several solutions aim to meet these trends, thus assisting users and satisfying their needs and preferences, such as virtual assistants and Location-Aware Recommender Systems (LARS), both in commercial and research literature. However, general purpose virtual assistants usually have to manage large domains, dealing with big amounts of data and online resources, losingfocus on more specific requirements and local information. On the other hand, traditional recommender systems are based on filtering techniques and contextual knowledge, and they usually do not rely on Natural Language Processing (NLP) features on users’ queries, which are useful to understand and contextualize users’ necessities on the spot. Therefore, comprehending the actual users’ information needs and other key information that can be included in the user query, such as geographical references, is a challenging task which is not yet fully accomplished by current state-of-the-art solutions. In this paper, we propose Paval (Location-Aware Virtual Personal Assistant 2), a semantic assisting engine for suggesting local points of interest (POIs) and services by analyzing users’ natural language queries, in order to estimate the information need and potential geographic references expressed by the users. The system exploits NLP and semantic techniques providing as output recommendations on local geolocated POIs and services which best match the users’ requests, retrieved by querying our semantic Km4City Knowledge Base. The proposed system is validated against the most popular virtual assistants, such as Google Assistant, Apple Siri and Microsoft Cortana, focusing the assessment on the request of geolocated POIs and services, showing very promising capabilities in successfully estimating the users’ information needs and multiple geographic references. © 2018 The Authors},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Massai, L. and Nesi, P. and Pantaleo, G.},
	year = {2019},
	keywords = {Geographic information retrieval, Geocoding, Geoparsing, Location-aware recommender systems, Natural language processing, Semantic web technologies, User-intent detection, Virtual personal assistants},
	pages = {70--85},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {040.pdf}
}

@book{gonzalez-medina_combination_2019,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Combination of {Semantic} {Localization} and {Conversational} {Skills} for {Assistive} {Robots}},
	volume = {855},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85057874949&doi=10.1007%2f978-3-319-99885-5_5&partnerID=40&md5=8cbdbb6ba34acbd4eda277fb6cc3a71b},
	abstract = {The recognition of objects and their features is a fundamental task for social robots that could be improved with the combination of different sources of information, such as the ones provided by visual or speech understanding systems. In this paper, we present a first approach to fusion semantic localization and conversational skills for social robots which may act as assistants. Our solution is based on a mobile robot that is able to detect and recognize objects from an environment and store them in its base of knowledge to later act as an assistant for any user who is searching for any object. In the conversation the robot tries to help the user to find a specific object depending of the location and the features of the object which is looking for. The proposal has been empirically evaluated within a research lab where the robot recognizes objects in the environment and the users require, by means of speech commands, finding suitable objects that are placed in the environment. © 2019, Springer Nature Switzerland AG.},
	author = {González-Medina, D. and Romero-González, C. and García-Varea, I.},
	year = {2019},
	keywords = {Artificial vision, Assistive robotics, Human-robot interaction, Smart homes, Speech recognition},
	annote = {Export Date: 18 September 2019},
	notes = {016.pdf}
}

@book{reis_creating_2019,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Creating {Weather} {Narratives}},
	volume = {11573 LNCS},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069637198&doi=10.1007%2f978-3-030-23563-5_25&partnerID=40&md5=97bd2f56afdf64caf986010bc9349847},
	abstract = {Information can be conveyed to the user by means of a narrative, modeled according to the user’s context. A case in point is the weather, which can be perceived differently and with distinct levels of importance according to the user’s context. For example, for a blind person, the weather is an important element to plan and move between locations. In fact, weather can make it very difficult or even impossible for a blind person to successfully negotiate a path and navigate from one place to another. To provide proper information, narrated and delivered according to the person’s context, this paper proposes a project for the creation of weather narratives, targeted at specific types of users and contexts. The proposal’s main objective is to add value to the data, acquired through the observation of weather systems, by interpreting that data, in order to identify relevant information and automatically create narratives, in a conversational way or with machine metadata language. These narratives should communicate specific aspects of the evolution of the weather systems in an efficient way, providing knowledge and insight in specific contexts and for specific purposes. Currently, there are several language generator’ systems, which automatically create weather forecast reports, based on previously processed and synthesized information. This paper, proposes a wider and more comprehensive approach to the weather systems phenomena, proposing a full process, from the raw data to a contextualized narration, thus providing a methodology and a tool that might be used for various contexts and weather systems. © 2019, Springer Nature Switzerland AG.},
	author = {Reis, A. and Liberato, M. and Paredes, H. and Martins, P. and Barroso, J.},
	year = {2019},
	annote = {Export Date: 18 September 2019},
	notes = {020.pdf}
}

@article{chen_cloud-based_2019,
	title = {Cloud-based dialog navigation agent system for service robots},
	volume = {31},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85067006484&doi=10.18494%2fSAM.2019.2326&partnerID=40&md5=6fb2af58718baed179e774a62883c246},
	doi = {10.18494/SAM.2019.2326},
	abstract = {The conventional human–robot interactions of robotic navigation systems must rely on strict language instructions and numerous button operations. In this study, a cloud-based dialog navigation agent (CDNA) system was designed for a campus navigation robot (CNR) that provides navigation services to students, school visitors, and people with impaired vision. The CDNA is based on a lightweight belief–desire–intention (BDI) software architecture (i.e., CellS, a cell-inspired efficient software framework), which is a goal-oriented and dynamic parallel framework. The proposed CDNA system has the following three primary functions: (1) conversational navigation service, (2) immediate path planning and path modification, and (3) location guide and place evaluation. The system can be applied to regional navigation guidance services such as campus tours. The CellS-based CDNA uses a natural language processing (NLP) technology to analyze the semantics of user statements and uses dialog to eliminate ambiguity in language to improve interaction with users. In this study, 15 items for three different navigation systems were evaluated, which demonstrated that the CDNA is advantageous in terms of interactivity and usability. The CellS-based CDNA can achieve an average speedup of 1.75 times in seven data sets. Therefore, the CDNA possesses the following advantages: high interactivity, high usability, and high performance. © MYU K.K.},
	number = {6},
	journal = {Sensors and Materials},
	author = {Chen, C.-H. and Wu, M.-C. and Wang, C.-C.},
	year = {2019},
	keywords = {Navigation, Natural language processing, Artificial intelligence, Automation, Robotics},
	pages = {1871--1891},
	annote = {Export Date: 18 September 2019},
	notes = {012.pdf}
}

@inproceedings{grazioso_linguistic_2018,
	title = {From linguistic linked open data to multimodal natural interaction: {A} case study},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85060155802&doi=10.1109%2fiV.2018.00060&partnerID=40&md5=73aebaecd4858dc7be9e53707492a0cc},
	doi = {10.1109/iV.2018.00060},
	abstract = {We present here the conversion of Linguistic Linked Open Data into Semantic Maps to be used to produce contents in a set of technological applications for Cultural Heritage. The paper describes the architectural data collection and annotation procedure adopted in the Cultural Heritage Orienting Multimodal Experiences (CHROME) project (PRIN 2015 funded by Italian University and Research Ministry). Such data will be used in Multimodal Dialogue Systems to obtain precise information about Architectural Heritage, by means of pointing gestures or verbal requests. In particular, we design conversational agents accessing fine-detailed semantic data linked to available 3D models of historical buildings. The starting point of our scientific approach is the Getty Vocabulary on Art \& Architecture Thesaurus, integrated with the Getty Thesaurus of Geographic Names (TGN) and the Union List of Artist Names (ULAN). These data are related to 3D mesh of the considered buildings in order to associate abstract concepts to architectural elements. In the field of 3D architectural investigation, a significant amount of research has been conducted to allow domain experts to represent semantic data while keeping spatial references. We will discuss how this will make it possible to support multimodal user interaction and generate Cultural Heritage presentations. © 2018 IEEE.},
	author = {Grazioso, M. and Cera, V. and Di Maro, M. and Origlia, A. and Cutugno, F.},
	year = {2018},
	keywords = {Data model, Multimodal interaction, vocabularies BASED ONTOLOGY},
	pages = {315--320},
	annote = {Export Date: 18 September 2019},
	notes = {024.pdf}
}

@inproceedings{bandara_cognitive_2018,
	title = {Cognitive {Spatial} {Representative} {Map} for {Interactive} {Conversational} {Model} of {Service} {Robot}},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85058072459&doi=10.1109%2fROMAN.2018.8525778&partnerID=40&md5=e6b8dcc0336a8e44f552b2733ed464bb},
	doi = {10.1109/ROMAN.2018.8525778},
	abstract = {Assistive robots are developed for supporting the daily activities of human beings to uplift the living standards. Assistive robots should be friendly, reliable, active, comprehensible and capable of creating interactive conversations with users in order to be a friendly companion for the human. Humans tend to include uncertain terms related to directions and distances to describe or express ideas. Furthermore, an assistive robot should be capable of analyzing and understanding the numerical meaning of uncertain terms for the purpose of creating a cognitive map in order to build friendly interactions between the user and the robot. Moreover, this paper proposes a method to identify the spatial relation between the objects in a given environment and describes such environments using uncertain terms related to spatial information based on a cognitive map created by the proposed system while having interactive conversations with the user. Conversation Management Module (CMM) and Spatial Information Processor (ISP) and Cognitive Map Creator (CMC) have been introduced in order to create interactive conversations while processing the uncertain information based on a cognitive map. Capabilities of the robot has been demonstrated and validated from the experimental result. © 2018 IEEE.},
	author = {Bandara, H.M.R.T. and Muthugala, M.A.V.J. and Jayasekara, A.G.B.P. and Chandima, D.P.},
	year = {2018},
	keywords = {cognitive map, conversation model, Human robot interaction, social robotics},
	pages = {686--691},
	annote = {Cited By :7},
	annote = {Export Date: 18 September 2019},
	notes = {015.pdf}
}

@inproceedings{prendergast_improving_2018,
	title = {Improving object disambiguation from natural language using empirical models},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85056622437&doi=10.1145%2f3242969.3243025&partnerID=40&md5=d489f2326688c6eb5aac9c20d85e318c},
	doi = {10.1145/3242969.3243025},
	abstract = {Robots, virtual assistants, and other intelligent agents need to effectively interpret verbal references to environmental objects in order to successfully interact and collaborate with humans in complex tasks. However, object disambiguation can be a challenging task due to ambiguities in natural language. To reduce uncertainty when describing an object, humans often use a combination of unique object features and locative prepositions-prepositional phrases that describe where an object is located relative to other features (i.e., reference objects) in a scene. We present a new system for object disambiguation in cluttered environments based on probabilistic models of unique object features and spatial relationships. Our work extends prior models of spatial relationship semantics by collecting and encoding empirical data from a series of crowdsourced studies to better understand how and when people use locative prepositions, how reference objects are chosen, and how to model prepositional geometry in 3D space (e.g., capturing distinctions between “next to” and “beside”). Our approach also introduces new techniques for responding to compound locative phrases of arbitrary complexity and proposes a new metric for disambiguation confidence. An experimental validation revealed our method can improve object disambiguation accuracy and performance over past approaches. © 2018 Copyright held by the owner/author(s).},
	author = {Prendergast, D. and Szafir, D.},
	year = {2018},
	keywords = {Grounding, Locative prepositions, Natural language disambiguation},
	pages = {477--485},
	annote = {Cited By :1},
	annote = {Export Date: 18 September 2019},
	notes = {026.pdf}
}

@inproceedings{atreja_citicafe:_2018,
	title = {Citicafe: {An} interactive interface for citizen engagement},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85045442883&doi=10.1145%2f3172944.3172955&partnerID=40&md5=6df3b5cded123d04397128d46bd284ad},
	doi = {10.1145/3172944.3172955},
	abstract = {Community engagement is a new and emerging trend in urban cities driven by the mission of developing responsible citizenship. Technology is playing a vital role in helping this mission. For example, over the last couple of years, there have been a plethora of social media avenues to report civic issues and complaints. We present one such contribution of technology, in terms of an intelligent platform, "Citicafe". The platform has a conversation based interface that enhances citizen engagement by enabling a direct communication with them. The platform ingests data from different sources, which is exploited by a virtual agent to enable informed interactions. It can help citizens to (a) report problems and (b) gather information related to civic issues for different locations and their neighborhoods. We report the results of a user study carried out to establish the effectiveness of our interface and draw a comparison with an existing platform. A detailed qualitative and quantitative analysis of the survey results shows a definite and statistically significant (p {\textless} 0.05) preference for our interface over the existing platform. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author = {Atreja, S. and Dumrewal, A. and Aggarwal, P. and Basu, A. and Mohapatra, P. and Dasgupta, G.B.},
	year = {2018},
	keywords = {Conversational agent, Citizen engagement, Clustering, CRF, Knowledge mining, Natural language, Social good, Topic modeling},
	pages = {617--628},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {011.pdf}
}

@inproceedings{gintner_improving_2018,
	title = {Improving reverse geocoding: {Localization} of blind pedestrians using conversational {UI}},
	volume = {2018-January},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85047131897&doi=10.1109%2fCogInfoCom.2017.8268232&partnerID=40&md5=4c488f2c855f48a2ba678e337f428382},
	doi = {10.1109/CogInfoCom.2017.8268232},
	abstract = {Geolocation services suffer from low precision in urban environments mainly due to sky occlusion and signal reflections from buildings, which is the key disadvantage of current navigation assistive AIDS for blind pedestrians. We designed a method that improves reverse geocoding to the level, at which the system can differentiate sides of the street where the blind user is traveling. By implementing a conversational user interface to ask the user about important landmarks and in combination with coarse location and heading, we achieved a usable, accessible and acceptable solution. The number of address points candidates can be reduced by 50 \% to those on the corresponding side of the street. A qualitative field study with six visually impaired participants confirmed acceptance by the user group and accessibility and usability of the method. © 2017 IEEE.},
	author = {Gintner, V. and Balata, J. and Boksansky, J. and Mikovec, Z.},
	year = {2018},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {027.pdf}
}

@book{klopfenstein_code_2018,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Code {Hunting} {Games}: {A} {Mixed} {Reality} {Multiplayer} {Treasure} {Hunt} {Through} a {Conversational} {Interface}},
	volume = {10750 LNCS},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85044452730&doi=10.1007%2f978-3-319-77547-0_14&partnerID=40&md5=641c3419d34401762d24c8830dda4854},
	abstract = {In this paper, we describe an online multi-player game that challenges players with abstract coding puzzles that are tied to a geographical location. The proposed system transposes the classical scheme of “treasure hunt” games into a mixed-reality game, where players must physically move in order to advance in the game, while at the same time interacting with a chatbot through an online messaging system. The implementation of the online game is described in detail and an overview of different deployments of the system is given, including a large-scale deployment during the European CodeWeek 2017. We discuss details of the proposed system, including lessons learned during the development and operation of the game. We also argue that mobile games like the one proposed can be successfully adopted for many different purposes, from entertainment to education. © 2018, Springer International Publishing AG, part of Springer Nature.},
	author = {Klopfenstein, L.C. and Delpriori, S. and Paolini, B.D. and Bogliolo, A.},
	year = {2018},
	keywords = {Bots, Conversational UI, Instant messaging, Mixed reality, Mobile gaming},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {014.pdf}
}

@inproceedings{gupta_raiden11iecsil-fire-2018:_2018,
	title = {Raiden11@{IECSIL}-{FIRE}-2018: {Named} entity recognition for {Indian} languages},
	volume = {2266},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85058625003&partnerID=40&md5=ae47d3f1078faaa810c44f77dddbfe03},
	abstract = {This paper presents our solution for the Named Entity Recognition (NER) task for the Information Extractor for Conversational Systems in Indian Languages challenge (IECSIL) [5] of the FIRE 2018 conference. A subset of the Information Extraction (IE) task, NER is a key to extract information and semantics of the text from unstructured data. The objective of NER is the identification and classification of every word or token in a document into predefined categories such as names of person, location, organization, etc. For this challenge the dataset provided by IECSIL [4] comprised of multilingual text of various Indian languages like Hindi, Tamil, Malayalam, Telugu, and Kannada. We mainly focus on the identification and classification of named entities belonging to nine categories like Name, Location, Datenum, etc. We tried linear models like Naive Bayes and SVM, and also a simple Neural Network to solve this problem. The best results are achieved by the simple neural network with an accuracy of 90.33\% for all languages combined. This indicates that different advanced neural networks could be possible solutions to further improve this accuracy. © 2018 CEUR-WS. All Rights Reserved.},
	author = {Gupta, A. and Ayyar, M. and Singh, A.K. and Shah, R.R.},
	year = {2018},
	keywords = {Information Extraction, Named Entity Recognition, Neural Networks, Word Embeddings},
	pages = {174--186},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {043.pdf}
}

@inproceedings{eiris-pereira_building_2018,
	title = {Building intelligent virtual agents as conversational partners in digital construction sites},
	volume = {2018-April},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85048696981&doi=10.1061%2f9780784481264.020&partnerID=40&md5=edf0fcd2b084fbc25d613bc486d957d5},
	doi = {10.1061/9780784481264.020},
	abstract = {Construction-related professionals are required to interact constantly on a variety of complex spatiotemporal occasions with several groups of people (e.g., architects, engineers, other construction professionals). The efficiency and effectiveness of the construction process strongly depends on the successful communication of information between all the functional groups involved in the different contexts of construction projects. However, lack of exposure to construction processes and construction professionals is widely observed in construction graduates. This lack of exposure results in deficient understanding of construction domain concepts in relation to real world problems. The use of intelligent virtual agents powered by building information modeling (BIM)-based virtual environments provides opportunities to incorporate conversational practices into classroom teaching. By employing these virtual interactions, a consequence-free environment that is controllable, representative, and repeatable can be achieved, with the benefit of providing constant real-time feedback to students. In such digital settings, students can observe spatiotemporal dependent occasions and communicate with other virtual professionals to obtain a better understanding of the construction events. In this paper, the technical workflow to create this virtual communication platform will be described in detail. This workflow includes (1) procedure of generating and authoring a virtual agent with its corresponding layers of information, (2) the creation process of the virtual environment that corresponds the conversational contexts using BIM-based technologies, and (3) the integration of the intelligent virtual agent and the digital site into a working platform. A case study is used to illustrate the conversational practice platform workflow for a high-risk caught-in or -between hazard scenario. © 2018 American Society of Civil Engineers (ASCE). All rights reserved.},
	author = {Eiris-Pereira, R. and Gheisari, M.},
	year = {2018},
	pages = {200--209},
	annote = {Export Date: 18 September 2019},
	notes = {010.pdf}
}

@inproceedings{thenmozhi_ssn_nlpiecsil-fire-2018:_2018,
	title = {{SSN}\_NLp@{IECSIL}-{FIRE}-2018: {Deep} learning approach to named entity recognition and relation extraction for conversational systems in {Indian} languages},
	volume = {2266},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85058656294&partnerID=40&md5=0cc6cd009ed11dfc58b167e8edad7a5b},
	abstract = {Named Entity Recognition (NER) focuses on the classification of proper nouns into the generic named entities (NE) such as person\_names, organizations, locations, currency and dates. NER has several applications like conversation systems, machine translation, automatic summarization and question answering. Relation Extraction (RE) is an information extraction process used to identify the relationship between NEs. RE is very important in applications like short answer grading, conversation systems, question answering and ontology learning. NER and RE in Indian languages are difficult tasks due to their agglutinative nature and rich morphological structure. Further, developing language independent framework that supports all Indian Languages is a challenging task. In this paper, we present a deep learning methodology for both NER and RE in five Indian languages namely Hindi, Kannada, Malayalam, Tamil and Telugu. We proposed a common approach that works for both NER and RE tasks. We have used neural machine translation architecture to implement our methodology for these tasks. Our approach was evaluated using the data set given by IECSIL@FIRE2018 shared task. We have evaluated on two sets of data for NER task and obtained the accuracies as 94.41\%, 95.23\%, 95.97\% and 96.02\% for the four variations on pre-evaluation test set and 95.9\%, 95.85\% and 95.05\% for the three runs on final-evaluation test set. Also, for RE task, we have obtained the accuracies as 56.19\%, 60.74\%, 60.7\%, 75.43\% and 79.11\% for our five variations on pre-evaluation test set and 79.44\%, 76.01\% and 61.11\% for Run 1, Run 2 and Run 3 respectively on final-evaluation test set. © 2018 CEUR-WS. All Rights Reserved.},
	author = {Thenmozhi, D. and Senthil Kumar, B. and Aravindan, C.},
	year = {2018},
	keywords = {Information Extraction, Deep Learning, Indian Languages, Named Entity Recognition (NER), Relation Extraction, Text mining},
	pages = {187--201},
	annote = {Export Date: 18 September 2019},
	notes = {046.pdf}
}

@article{bartie_dialogue_2018,
	title = {A dialogue based mobile virtual assistant for tourists: {The} {SpaceBook} {Project}},
	volume = {67},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85033448434&doi=10.1016%2fj.compenvurbsys.2017.09.010&partnerID=40&md5=dac9e4856b5c6eb1d54c26ef9699876f},
	doi = {10.1016/j.compenvurbsys.2017.09.010},
	abstract = {Ubiquitous mobile computing offers innovative approaches in the delivery of information that can facilitate free roaming of the city, informing and guiding the tourist as the city unfolds before them. However making frequent visual reference to mobile devices can be distracting, the user having to interact via a small screen thus disrupting the explorative experience. This research reports on an EU funded project, SpaceBook, that explored the utility of a hands-free, eyes-free virtual tour guide, that could answer questions through a spoken dialogue user interface and notify the user of interesting features in view while guiding the tourist to various destinations. Visibility modelling was carried out in real-time based on a LiDAR sourced digital surface model, fused with a variety of map and crowd sourced datasets (e.g. Ordnance Survey, OpenStreetMap, Flickr, Foursquare) to establish the most interesting landmarks visible from the user's location at any given moment. A number of variations of the SpaceBook system were trialled in Edinburgh (Scotland). The research highlighted the pleasure derived from this novel form of interaction and revealed the complexity of prioritising route guidance instruction alongside identification, description and embellishment of landmark information – there being a delicate balance between the level of information ‘pushed’ to the user, and the user's requests for further information. Among a number of challenges, were issues regarding the fidelity of spatial data and positioning information required for pedestrian based systems – the pedestrian having much greater freedom of movement than vehicles. © 2017},
	journal = {Computers, Environment and Urban Systems},
	author = {Bartie, P. and Mackaness, W. and Lemon, O. and Dalmas, T. and Janarthanam, S. and Hill, R.L. and Dickinson, A. and Liu, X.},
	year = {2018},
	keywords = {Location based service, Spoken Dialogue System, Viewshed, Virtual city guide},
	pages = {110--123},
	annote = {Cited By :5},
	annote = {Export Date: 18 September 2019},
	notes = {001.pdf}
}

@inproceedings{agarwal_remembering_2017,
	title = {Remembering what you said: {Semantic} personalized memory for personal digital assistants},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85023766167&doi=10.1109%2fICASSP.2017.7953275&partnerID=40&md5=22fb7382dacc64a9975052dccd1db802},
	doi = {10.1109/ICASSP.2017.7953275},
	abstract = {Personal digital assistants are designed to assist users in easy information retrieval or execute the tasks they are interested in. The conversational medium implies an additional level of intelligence but typically these systems do not support any reference to the user's past interactions. We propose a domain-agnostic approach that enables the system to address queries referring to the past by using an information retrieval approach to rank various entities for a given query. We also add semantic enrichment to the recall process by augmenting the entities with information from a knowledge graph and leverage that in the retrieval process. We mined user interactions for the Cortana digital assistant to extract queries with location and business entities and show that our technique can achieve an accuracy of 89.8\% for such recall queries. © 2017 IEEE.},
	author = {Agarwal, V. and Khan, O.Z. and Sarikaya, R.},
	year = {2017},
	keywords = {dialog management, information retrieval, Personal digital assistants, referring expressions, spoken language understanding},
	pages = {5835--5839},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {044.pdf}
}

@inproceedings{zhu_using_2017,
	title = {Using knowledge graph and search query click logs in statistical language model for speech recognition},
	volume = {2017-August},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85039169288&doi=10.21437%2fInterspeech.2017-1790&partnerID=40&md5=bb81a2ee3efba0f484921d9a29764a9d},
	doi = {10.21437/Interspeech.2017-1790},
	abstract = {This paper demonstrates how Knowledge Graph (KG) and Search Query Click Logs (SQCL) can be leveraged in statistical language models to improve named entity recognition for online speech recognition systems. Due to the missing in the training data, some named entities may be recognized as other common words that have the similar pronunciation. KG and SQCL cover comprehensive and fresh named entities and queries that can be used to mitigate the wrong recognition. First, all the entities located in the same area in KG are clustered together, and the queries that contain the entity names are selected from SQCL as the training data of a geographical statistical language model for each entity cluster. These geographical language models make the unseen named entities less likely to occur during the model training, and can be dynamically switched according to the user location in the recognition phase. Second, if any named entities are identified in the previous utterances within a conversational dialog, the probability of the n-best word sequence paths that contain their related entities will be increased for the current utterance by utilizing the entity relationships from KG and SQCL. This way can leverage the long-Term contexts within the dialog. Experiments for the proposed approach on voice queries from a spoken dialog system yielded a 12.5\% relative perplexity reduction in the language model measurement, and a 1.1\% absolute word error rate reduction in the speech recognition measurement. Copyright © 2017 ISCA.},
	author = {Zhu, W.},
	year = {2017},
	keywords = {knowledge graph, named entity recognition, search query click log, speech recognition, statistical language model},
	pages = {2735--2738},
	annote = {Cited By :2},
	annote = {Export Date: 18 September 2019},
	notes = {048.pdf}
}

@article{garrido_smart_2017,
	title = {Smart tourist information points by combining agents, semantics and {AI} techniques},
	volume = {14},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85011700322&doi=10.2298%2fCSIS150410029G&partnerID=40&md5=d1e50fd3366245f4b55a080ea0a1fe0c},
	doi = {10.2298/CSIS150410029G},
	abstract = {The tourism sector in the province of Teruel (Aragon, Spain) is increasing rapidly. Although the number of domestic and foreign tourists is continuously growing, there are some tourist attractions spread over a wide geographical area, which are only visited by a few people at specific times of the year. Additionally, having human tourist guides everywhere and speaking different languages is unfeasible. An integrated solution based on smart and interactive Embodied Conversational Agents (ECAs) tourist guides combined with ontologies would overcome this problem. This paper presents a smart tourist information points approach which gathers tourism information about Teruel, structured according to a novel lightweight ontology built on OWL (Ontology Web Language), known as TITERIA (Touristic Information of TEruel for Intelligent Agents). Our proposal, which combines TITERIA with the Maxine platform, is capable of responding appropriately to the users thanks to its Artificial Intelligence Modeling Language (AIML) database and the AI techniques added to Maxine. Preliminary results indicate that our prototype is able to inform users about interesting topics, as well as to propose other related information, allowing them to acquire a complete information about any issue. Furthermore, users can directly talk with an artificial actor making communication much more natural and closer. © 2017, ComSIS Consortium. All rights reserved.},
	number = {1},
	journal = {Computer Science and Information Systems},
	author = {Garrido, P. and Barrachina, J. and Martinez, F.J. and Seron, F.J.},
	year = {2017},
	keywords = {Virtual human, Embodied conversational agent, Ontology, Tourism},
	pages = {1--23},
	annote = {Cited By :7},
	annote = {Export Date: 18 September 2019},
	notes = {045.pdf}
}

@inproceedings{signoretti_trip_2015,
	title = {Trip 4 {All}: {A} {Gamified} {App} to {Provide} a {New} {Way} to {Elderly} {People} to {Travel}},
	volume = {67},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84962809876&doi=10.1016%2fj.procs.2015.09.274&partnerID=40&md5=5095f1c2d76c0462257f1def073c3445},
	doi = {10.1016/j.procs.2015.09.274},
	abstract = {Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy to use products, developed using an elderly centered methodology. The senior tourism is a market in expansion and the old travelers need new and innovative technologies to help and support their trips. These technologies should contribute to a fun and safe experience, while promoting feelings of pleasure and self realization. In this paper we follow this design approach and put it to the test in developing the "Trip 4 All"(T4A), an application that works as a gamified virtual assistant to the elderly during a walking tourist visit. The gamified interaction with the visited environment intend to improve motivation to accomplish the visit and make the content absorption more fun and easier. The T4A works on georeferenced maps where the users' geoposition is a trigger to launch storytelling content and/or challenges based on the aspects of the visited site as such: geographical, art, religious, historic, cultural and human. The success in the challenges give the user prizes, new resources and abilities to try more complex challenges that brings more valuable prizes and so on. Furthermore, the proposed application intend to work as a companion that provides self confidence, support and social integration to elderly tourists. © 2015 The Authors.},
	author = {Signoretti, A. and Martins, A.I. and Almeida, N. and Vieira, D. and Rosa, A.F. and Costa, C.M.M. and Texeira, A.},
	year = {2015},
	keywords = {Active Aging, Elderly-centred design, Iterative development method, Mobile application evaluation},
	pages = {301--311},
	annote = {Cited By :6},
	annote = {Export Date: 18 September 2019},
	notes = {047.pdf}
}

@article{cai_modeling_2013,
	title = {Modeling and communicating the conceptual intent of geo-analytical tasks for human-{GIS} interaction},
	volume = {17},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84892979721&doi=10.1111%2ftgis.12040&partnerID=40&md5=6fa669fd94d8e79bc084794d11d0321e},
	doi = {10.1111/tgis.12040},
	abstract = {One of the fundamental issues of geographical information science is to design GIS interfaces and functionalities in a way that is easy to understand, teach, and use. Unfortunately, current geographical information systems (including ArcGIS) remains very difficult to use as spatial analysis tools, because they organize and expose functionalities according to GIS data structures and processing algorithms. As a result, GIS interfaces are conceptually confusing, cognitively complex, and semantically disconnected from the way human reason about spatial analytical activities. In this article, we propose an approach that structures GIS analytical functions based on the notion of "analytical intent". We describe an experiment that replaces ArcGIS desktop interface with a conversational interface, to enable mixed-initiative user-system interactions at the level of analytical intentions. We initially focus on the subset of GIS functions that are relevant to "finding what's inside" as described by Mitchell, but the general principles apply to other types of spatial analysis. This work demonstrates the feasibility of delegating some spatial thinking tasks to computational agents, and also raises future research questions that are key to building a better theory of spatial thinking with GIS. © 2013 John Wiley \& Sons Ltd.},
	number = {3},
	journal = {Transactions in GIS},
	author = {Cai, G. and Yu, B. and Chen, D.},
	year = {2013},
	pages = {353--368},
	annote = {Cited By :1},
	annote = {Export Date: 18 September 2019},
	notes = {035.pdf}
}

@inproceedings{janarthanam_conversational_2012,
	title = {Conversational natural language interaction for place-related knowledge acquisition},
	volume = {881},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84893238858&partnerID=40&md5=512ad7516542ee169c2713183fe0d74a},
	abstract = {We focus on the problems of using Natural Language interaction to support pedestrians in their place-related knowledge acquisition. Our case study for this discussion is a smartphone-based Natural Language interface that allows users to acquire spatial and cultural knowledge of a city. The framework consists of a spoken dialogue-based information system and a smartphone client. The system is novel in combining geographic information system (GIS) modules such as a visibility engine with a question-answering (QA) system. Users can use the smart-phone client to engage in a variety of interleaved conversations such as navigating from A to B, using the QA functionality to learn more about points of interest (PoI) nearby, and searching for amenities and tourist attractions. This system explores a variety of research questions involving Natural Language interaction for acquisition of knowledge about space and place.},
	author = {Janarthanam, S. and Lemon, O. and Liu, X. and Bartie, P. and Mackaness, W. and Dalmas, T. and Goetze, J.},
	year = {2012},
	pages = {33--38},
	annote = {Export Date: 18 September 2019},
	notes = {018.pdf}
}

@inproceedings{cuayahuitl_optimizing_2011,
	title = {Optimizing situated dialogue management in unknown environments},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865724935&partnerID=40&md5=b267e60693ea0410300d23c84fd3291f},
	abstract = {We present a conversational learning agent that helps users navigate through complex and challenging spatial environments. The agent exhibits adaptive behaviour by learning spatiallyaware dialogue actions while the user carries out the navigation task. To this end, we use Hierarchical Reinforcement Learning with relational representations to efficiently optimize dialogue actions tightly-coupled with spatial ones, and Bayesian networks to model the user's beliefs of the navigation environment. Since these beliefs are continuously changing, we induce the agent's behaviour in real time. Experimental results, using simulation, are encouraging by showing efficient adaptation to the user's navigation knowledge, specifically to the generated route and the intermediate locations to negotiate with the user. Copyright © 2011 ISCA.},
	author = {Cuayáhuitl, H. and Dethlefs, N.},
	year = {2011},
	keywords = {Situated interaction, Bayesian networks, Hierarchical control, Reinforcement learning, Spoken dialogue systems},
	pages = {1009--1012},
	annote = {Cited By :8},
	annote = {Export Date: 18 September 2019},
	notes = {039.pdf}
}

@inproceedings{dobnik_human_2010,
	title = {Human evaluation of robot-generated spatial descriptions},
	volume = {620},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84889041228&partnerID=40&md5=152472d73d0a980b757a28799c6864f6},
	abstract = {We describe a system where the semantics of spatial referential expressions have been automatically learned by finding mappings between symbolic natural language descriptions of the environment and non-symbolic representations from the sensory data of a mobile robot used for localisation and map building (SLAM). Although the success of learning can be measured by examining classifier performance on held-out data, this does not in itself guarantee that the descriptions generated will be natural and informative for a human observer. In this paper we describe the results of an evaluation of our embodied robotic system by human observers. Key words: spatial expressions, machine learning, mobile robots, embod- ied multi-modal conversational agents, evaluation.},
	author = {Dobnik, S. and Pulman, S.G.},
	year = {2010},
	pages = {25--32},
	annote = {Export Date: 18 September 2019},
	notes = {025.pdf}
}

@book{sanchez-pi_multi-agent_2010,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Multi-agent system ({MAS}) applications in ambient intelligence ({AmI}) environments},
	volume = {71},
	url = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84892442656&doi=10.1007%2f978-3-642-12433-4_58&partnerID=40&md5=424dbc8fc1333579a934a083ef47acd3},
	abstract = {Research in context-aware systems has been moving towards reusable and adaptable architectures for managing more advanced human-computer interfaces. Ambient. Intelligence (AmI) investigates computer-based services, which are ubiquitous and based on a variety of objects and devices. Their intelligent and intuitive interfaces act as mediators through which people can interact with the ambient environment. In this paper we present an agent-based architecture which supports the execution of agents in AmI environments. Two case studies are also presented, an airport information system and a railway information system, which uses spoken conversational agents to respond to the user’s requests using the contextual information that includes the location information of the user. © Springer-Verlag Berlin Heidelberg 2010.},
	author = {Sánchez-Pi, N. and Mangina, E. and Carbó, J. and Molina, J.M.},
	year = {2010},
	keywords = {Mobile context-aware systems, Multi-agent systems, Services oriented architectures},
	annote = {Cited By :3},
	annote = {Export Date: 18 September 2019},
	notes = {037.pdf}
}

@article{tsai_ask_nodate,
	title = {Ask {Diana}: {A} {Keyword}-{Based} {Chatbot} {System} for {Water}-{Related} {Disaster} {Management}},
	volume = {11},
	doi = {https://doi.org/10.3390/w11020234},
	abstract = {This research developed a keyword-based chatbot system, Ask Diana, for water-related disaster management. Disaster management has been considered difficult and tedious due to the complex characteristics of disaster-related data. To deal with this problem, this research developed a chatbot system with a water-related disaster database, a user intent mechanism, and an intuitive mobile-device-based user interface. With such a system, users are able to access important data or information they need for decision making by directly asking the proposed chatbot or operating the image-based menus. The system was validated through a usability test and a six-month field test. The results demonstrated that Ask Diana can help related personnel access disaster data intuitively and develop corresponding response strategies efficiently.},
	language = {English},
	number = {2},
	journal = {Water},
	author = {Tsai, Meng-Han and Chen, James Yichun and Kang, Shih-Chung},
	pages = {234},
	notes = {101.pdf}
}