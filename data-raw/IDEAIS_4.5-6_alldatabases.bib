
@inproceedings{hamzei_place_2020,
	series = {Lecture {Notes} in {Geoinformation} and {Cartography}},
	title = {Place questions and human-generated answers: {A} data analysis approach},
	booktitle={The Annual International Conference on Geographic Information Science},
	abstract = {This paper investigates place-related questions submitted to search systems and their human-generated answers. Place-based search is motivated by the need to identify places matching some criteria, to identify them in space or relative to other places, or to characterize the qualities of such places. Human place-related questions have thus far been insufficiently studied and differ strongly from typical keyword queries. They thus challenge today’s search engines providing only rudimentary geographic information retrieval support. We undertake an analysis of the patterns in place-based questions using a large-scale dataset of questions/answers, MS MARCO V2.1. The results of this study reveal patterns that can inform the design of conversational search systems and in-situ assistance systems, such as autonomous vehicles.},
	author = {Hamzei, E. and Li, H. and Vasardani, M. and Baldwin, T. and Winter, S. and Tomko, M.},
	year = {2020},
	keywords = {Geographic information retrieval, Geographic questions, Query classification, Question answering systems, Web search queries},
	notes = {041.pdf}
}

@article{subramani_robust_2019,
	title = {A robust artificial intelligence: smart indoor positioning system},
	volume = {8},
	doi = {10.35940/ijitee.A1001.0881019},
	abstract = {Over the previous few centuries, technology has converted massively from being a desktop personal computer to handheld mobile phones, with lower energy consumption of raw computing power. This computability is now incorporated with other systems as well as isolated to a single device. This paradigm was first noted in cyber-physical systems with the introduction of cloud services. The evolution of Artificial Intelligence(AI) with cloud computing and the importance of this field in human life, induce us to make simple and efficient talkative assistant robot for indoor navigation. The navigation system in outdoor typically rely upon Global Positioning System (GPS) but the indoor navigation systems have to rely on different technologies, as GPS signals cannot be received indoors. Thus, several technologies have been proposed and implemented over the past decade to improve navigation in indoors. But they were costly and less effective. Therefore, we have proposed a system that assists humans to find their location in a conversational manner. The suggested system was constructed by introducing the advantages of a personal assistant device, Amazon Alexa, the cloud services of Amazon and its voice services for indoor navigation. A Raspberry Pi 3 Model B is used as the element of the hardware to provide our system with intelligent characteristics. You can trigger the speech service using the "Alexa" keyword. Using the voice command, the skill/ application we created can be initiated. It operates a script on the cloud once Alexa is enabled, which runs a subroutine on the Raspberry Pi 3 in-turn to provide a path for that specific place. Once the Raspberry Pi calculation is finished, it sends the message back to Alexa. Alexa transforms the text into a voice and informs the user path.},
	number = {10},
	journal = {International Journal of Innovative Technology and Exploring Engineering},
	author = {Subramani, B. and Deepika, S.P.},
	year = {2019},
	keywords = {Artificial Intelligence, Global Positioning System, Raspberry Pi, Voice command},
	pages = {1393--1398},
	notes = {004.pdf}
}

@inproceedings{ali_design_2019,
	title = {Design of seamless multi-modal interaction framework for intelligent virtual agents in wearable mixed reality environment},
	doi = {10.1145/3328756.3328758},
	booktitle={Proceedings of the 32nd International Conference on Computer Animation and Social Agents},
	abstract = {In this paper, we present the design of a multimodal interaction framework for intelligent virtual agents in wearable mixed reality environments, especially for interactive applications at museums, botanical gardens, and similar places. These places need engaging and no-repetitive digital content delivery to maximize user involvement. An intelligent virtual agent is a promising mode for both purposes. Premises of framework is wearable mixed reality provided by MR devices supporting spatial mapping. We envisioned a seamless interaction framework by integrating potential features of spatial mapping, virtual character animations, speech recognition, gazing, domain-specific chatbot and object recognition to enhance virtual experiences and communication between users and virtual agents. By applying a modular approach and deploying computationally intensive modules on cloud-platform, we achieved a seamless virtual experience in a device with limited resources. Human-like gaze and speech interaction with a virtual agent made it more interactive. Automated mapping of body animations with the content of a speech made it more engaging. In our tests, the virtual agents responded within 2-4 seconds after the user query. The strength of the framework is flexibility and adaptability. It can be adapted to any wearable MR device supporting spatial mapping.},
	author = {Ali, G. and Le, H.-Q. and Kim, J. and Hwang, S.-W. and Hwang, J.-I.},
	year = {2019},
	pages = {47--52},
	notes = {021.pdf}
}

@inproceedings{acer_hyper-local_2019,
	title = {On hyper-local conversational agents in urban settings},
	doi = {10.1145/3325426.3329949},
	booktitle={Proceedings of the 5th ACM Workshop on Mobile Systems for Computational Social Science},
	abstract = {Conversational agents are increasingly becoming digital partners in our everyday computational experiences. Although rich, and fresh in content, these agents are completely oblivious to users' locality beyond geospatial weather and traffic conditions. In this position statement, we envisage a brand-new class of conversational agents that are hyper-local, embedded deeply in a local neighbourhood, e.g., at urban landmarks - providing rich, purposeful, detail, and in some cases playful information relevant to a neighbourhood. By design, these agents are spatially constrained, and one can only interact with them once she is in close vicinity at street-level granularity. Learning from quantitative (n = 1992) and qualitative (n = 21) studies, we identify a set of information that these agents must accommodate. Finally, we discuss the technical architecture of this class of conversational agents that leverages covert communication channel, edge AI and on-body devices for offering such hyper-local information access.},
	author = {Acer, U.G. and Van Den Broeck, M. and Kawsar, F.},
	year = {2019},
	keywords = {Citizen Engagement, Conversational Agent, Edge AI, Spontaneous Interaction},
	pages = {1--4},
	notes = {038.pdf}
}

@inproceedings{sardella_approach_2019,
	series = {Communications in {Computer} and {Information} {Science}},
	title = {An {Approach} to {Conversational} {Recommendation} of {Restaurants}},
	booktitle={International Conference on Human-Computer Interaction},
  	pages={123--130},
	abstract = {In this paper, we propose an approach based on the integration of a chatbot module, a location-based service, and a recommendation algorithm. This approach has been deployed for restaurant recommendation, tested on a sample of 50 real users, and compared with some state-of-the-art algorithms. The preliminary experimental results showed the benefits of the proposed approach in terms of performance. An ANOVA test enabled us to verify the statistical significance of the obtained findings.},
	author = {Sardella, N. and Biancalana, C. and Micarelli, A. and Sansonetti, G.},
	year = {2019},
	keywords = {Cold-start, Conversational recommender systems, Location-based services},
	notes = {007.pdf}
}

@inproceedings{anelli_anna:_2019,
	title = {Anna: {A} virtual assistant to interact with puglia digital library (discussion paper)},
	volume = {2400},
	booktitle = {Proceedings of the 27th Italian Symposium on Advanced Database Systems},
	abstract = {In the last years, a huge amount of data has been released by private and public bodies as Linked Open Data. By their inner nature, these data contain rich semantic information that can be automatically processed by software agents and explored by humans via visual tools or structured SPARQL queries. Although they result useful for many tasks, these latter approaches miss the simplicity of the interfaces based on interactions via natural language implemented in modern virtual assistants. In this paper, we present a system able to assist the user in exploring the knowledge exposed by the Puglia Digital Library containing information and data associated with digital goods related to the Apulia region in Italy. We show how to interact with the Digital Library by means of a virtual assistant and how, thanks to its publication as Linked Open Data, it is possible to easily integrate it on-the-fly with external knowledge sources such as geographical ones.},
	author = {Anelli, V.W. and Di Noia, T. and Di Sciascio, E. and Ragone, A.},
	year = {2019},
	keywords = {Chatbot, Digital Libraries, Linked Open Data, Vocal Assistant},
	notes = {008.pdf}
}

@article{massai_paval:_2019,
	title = {{PAVAL}: {A} location-aware virtual personal assistant for retrieving geolocated points of interest and location-based services},
	volume = {77},
	doi = {10.1016/j.engappai.2018.09.013},
	abstract = {Today most of the users on the move require contextualized local and georeferenced information. Several solutions aim to meet these trends, thus assisting users and satisfying their needs and preferences, such as virtual assistants and Location-Aware Recommender Systems (LARS), both in commercial and research literature. However, general purpose virtual assistants usually have to manage large domains, dealing with big amounts of data and online resources, losingfocus on more specific requirements and local information. On the other hand, traditional recommender systems are based on filtering techniques and contextual knowledge, and they usually do not rely on Natural Language Processing (NLP) features on users’ queries, which are useful to understand and contextualize users’ necessities on the spot. Therefore, comprehending the actual users’ information needs and other key information that can be included in the user query, such as geographical references, is a challenging task which is not yet fully accomplished by current state-of-the-art solutions. In this paper, we propose Paval (Location-Aware Virtual Personal Assistant 2), a semantic assisting engine for suggesting local points of interest (POIs) and services by analyzing users’ natural language queries, in order to estimate the information need and potential geographic references expressed by the users. The system exploits NLP and semantic techniques providing as output recommendations on local geolocated POIs and services which best match the users’ requests, retrieved by querying our semantic Km4City Knowledge Base. The proposed system is validated against the most popular virtual assistants, such as Google Assistant, Apple Siri and Microsoft Cortana, focusing the assessment on the request of geolocated POIs and services, showing very promising capabilities in successfully estimating the users’ information needs and multiple geographic references.},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Massai, L. and Nesi, P. and Pantaleo, G.},
	year = {2019},
	keywords = {Geographic information retrieval, Geocoding, Geoparsing, Location-aware recommender systems, Natural language processing, Semantic web technologies, User-intent detection, Virtual personal assistants},
	pages = {70--85},
	notes = {040.pdf}
}

@inproceedings{gonzalez-medina_combination_2019,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Combination of {Semantic} {Localization} and {Conversational} {Skills} for {Assistive} {Robots}},
	volume = {855},
	booktitle={Proceedings of the 19th International Workshop of Physical Agents},
	abstract = {The recognition of objects and their features is a fundamental task for social robots that could be improved with the combination of different sources of information, such as the ones provided by visual or speech understanding systems. In this paper, we present a first approach to fusion semantic localization and conversational skills for social robots which may act as assistants. Our solution is based on a mobile robot that is able to detect and recognize objects from an environment and store them in its base of knowledge to later act as an assistant for any user who is searching for any object. In the conversation the robot tries to help the user to find a specific object depending of the location and the features of the object which is looking for. The proposal has been empirically evaluated within a research lab where the robot recognizes objects in the environment and the users require, by means of speech commands, finding suitable objects that are placed in the environment.},
  	pages={56--69},
	author = {González-Medina, D. and Romero-González, C. and García-Varea, I.},
	year = {2019},
	keywords = {Artificial vision, Assistive robotics, Human-robot interaction, Smart homes, Speech recognition},
	notes = {016.pdf}
}

@inproceedings{reis_creating_2019,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Creating {Weather} {Narratives}},
	volume = {11573 LNCS},
	booktitle={International Conference on Human-Computer Interaction},
	abstract = {Information can be conveyed to the user by means of a narrative, modeled according to the user’s context. A case in point is the weather, which can be perceived differently and with distinct levels of importance according to the user’s context. For example, for a blind person, the weather is an important element to plan and move between locations. In fact, weather can make it very difficult or even impossible for a blind person to successfully negotiate a path and navigate from one place to another. To provide proper information, narrated and delivered according to the person’s context, this paper proposes a project for the creation of weather narratives, targeted at specific types of users and contexts. The proposal’s main objective is to add value to the data, acquired through the observation of weather systems, by interpreting that data, in order to identify relevant information and automatically create narratives, in a conversational way or with machine metadata language. These narratives should communicate specific aspects of the evolution of the weather systems in an efficient way, providing knowledge and insight in specific contexts and for specific purposes. Currently, there are several language generator’ systems, which automatically create weather forecast reports, based on previously processed and synthesized information. This paper, proposes a wider and more comprehensive approach to the weather systems phenomena, proposing a full process, from the raw data to a contextualized narration, thus providing a methodology and a tool that might be used for various contexts and weather systems.},
	author = {Reis, A. and Liberato, M. and Paredes, H. and Martins, P. and Barroso, J.},
	year = {2019},
	pages={312--322},
	notes = {020.pdf}
}

@article{chen_cloud-based_2019,
	title = {Cloud-based dialog navigation agent system for service robots},
	volume = {31},
	doi = {10.18494/SAM.2019.2326},
	abstract = {The conventional human–robot interactions of robotic navigation systems must rely on strict language instructions and numerous button operations. In this study, a cloud-based dialog navigation agent (CDNA) system was designed for a campus navigation robot (CNR) that provides navigation services to students, school visitors, and people with impaired vision. The CDNA is based on a lightweight belief–desire–intention (BDI) software architecture (i.e., CellS, a cell-inspired efficient software framework), which is a goal-oriented and dynamic parallel framework. The proposed CDNA system has the following three primary functions: (1) conversational navigation service, (2) immediate path planning and path modification, and (3) location guide and place evaluation. The system can be applied to regional navigation guidance services such as campus tours. The CellS-based CDNA uses a natural language processing (NLP) technology to analyze the semantics of user statements and uses dialog to eliminate ambiguity in language to improve interaction with users. In this study, 15 items for three different navigation systems were evaluated, which demonstrated that the CDNA is advantageous in terms of interactivity and usability. The CellS-based CDNA can achieve an average speedup of 1.75 times in seven data sets. Therefore, the CDNA possesses the following advantages: high interactivity, high usability, and high performance.},
	number = {6},
	journal = {Sensors and Materials},
	author = {Chen, C.-H. and Wu, M.-C. and Wang, C.-C.},
	year = {2019},
	keywords = {Navigation, Natural language processing, Artificial intelligence, Automation, Robotics},
	pages = {1871--1891},
	notes = {012.pdf}
}

@inproceedings{grazioso_linguistic_2018,
	title = {From linguistic linked open data to multimodal natural interaction: {A} case study},
	doi = {10.1109/iV.2018.00060},
	booktitle={22nd International Conference Information Visualisation},
	abstract = {We present here the conversion of Linguistic Linked Open Data into Semantic Maps to be used to produce contents in a set of technological applications for Cultural Heritage. The paper describes the architectural data collection and annotation procedure adopted in the Cultural Heritage Orienting Multimodal Experiences (CHROME) project (PRIN 2015 funded by Italian University and Research Ministry). Such data will be used in Multimodal Dialogue Systems to obtain precise information about Architectural Heritage, by means of pointing gestures or verbal requests. In particular, we design conversational agents accessing fine-detailed semantic data linked to available 3D models of historical buildings. The starting point of our scientific approach is the Getty Vocabulary on Art \& Architecture Thesaurus, integrated with the Getty Thesaurus of Geographic Names (TGN) and the Union List of Artist Names (ULAN). These data are related to 3D mesh of the considered buildings in order to associate abstract concepts to architectural elements. In the field of 3D architectural investigation, a significant amount of research has been conducted to allow domain experts to represent semantic data while keeping spatial references. We will discuss how this will make it possible to support multimodal user interaction and generate Cultural Heritage presentations.},
	author = {Grazioso, M. and Cera, V. and Di Maro, M. and Origlia, A. and Cutugno, F.},
	year = {2018},
	keywords = {Data model, Multimodal interaction, Vocabulary-based ontologies},
	pages = {315--320},
	notes = {024.pdf}
}

@inproceedings{bandara_cognitive_2018,
	title = {Cognitive {Spatial} {Representative} {Map} for {Interactive} {Conversational} {Model} of {Service} {Robot}},
	doi = {10.1109/ROMAN.2018.8525778},
	booktitle={27th IEEE International Symposium on Robot and Human Interactive Communication},
	abstract = {Assistive robots are developed for supporting the daily activities of human beings to uplift the living standards. Assistive robots should be friendly, reliable, active, comprehensible and capable of creating interactive conversations with users in order to be a friendly companion for the human. Humans tend to include uncertain terms related to directions and distances to describe or express ideas. Furthermore, an assistive robot should be capable of analyzing and understanding the numerical meaning of uncertain terms for the purpose of creating a cognitive map in order to build friendly interactions between the user and the robot. Moreover, this paper proposes a method to identify the spatial relation between the objects in a given environment and describes such environments using uncertain terms related to spatial information based on a cognitive map created by the proposed system while having interactive conversations with the user. Conversation Management Module (CMM) and Spatial Information Processor (ISP) and Cognitive Map Creator (CMC) have been introduced in order to create interactive conversations while processing the uncertain information based on a cognitive map. Capabilities of the robot has been demonstrated and validated from the experimental result.},
	author = {Bandara, H.M.R.T. and Muthugala, M.A.V.J. and Jayasekara, A.G.B.P. and Chandima, D.P.},
	year = {2018},
	keywords = {cognitive map, conversation model, Human robot interaction, social robotics},
	pages = {686--691},
	notes = {015.pdf}
}

@inproceedings{prendergast_improving_2018,
	title = {Improving object disambiguation from natural language using empirical models},
	doi = {10.1145/3242969.3243025},
	booktitle={Proceedings of the 20th ACM International Conference on Multimodal Interaction},
	abstract = {Robots, virtual assistants, and other intelligent agents need to effectively interpret verbal references to environmental objects in order to successfully interact and collaborate with humans in complex tasks. However, object disambiguation can be a challenging task due to ambiguities in natural language. To reduce uncertainty when describing an object, humans often use a combination of unique object features and locative prepositions-prepositional phrases that describe where an object is located relative to other features (i.e., reference objects) in a scene. We present a new system for object disambiguation in cluttered environments based on probabilistic models of unique object features and spatial relationships. Our work extends prior models of spatial relationship semantics by collecting and encoding empirical data from a series of crowdsourced studies to better understand how and when people use locative prepositions, how reference objects are chosen, and how to model prepositional geometry in 3D space (e.g., capturing distinctions between “next to” and “beside”). Our approach also introduces new techniques for responding to compound locative phrases of arbitrary complexity and proposes a new metric for disambiguation confidence. An experimental validation revealed our method can improve object disambiguation accuracy and performance over past approaches.},
	author = {Prendergast, D. and Szafir, D.},
	year = {2018},
	keywords = {Grounding, Locative prepositions, Natural language disambiguation},
	pages = {477--485},
	notes = {026.pdf}
}

@inproceedings{atreja_citicafe:_2018,
	title = {Citicafe: {An} interactive interface for citizen engagement},
	doi = {10.1145/3172944.3172955},
	booktitle={23rd International Conference on Intelligent User Interfaces},
	abstract = {Community engagement is a new and emerging trend in urban cities driven by the mission of developing responsible citizenship. Technology is playing a vital role in helping this mission. For example, over the last couple of years, there have been a plethora of social media avenues to report civic issues and complaints. We present one such contribution of technology, in terms of an intelligent platform, "Citicafe". The platform has a conversation based interface that enhances citizen engagement by enabling a direct communication with them. The platform ingests data from different sources, which is exploited by a virtual agent to enable informed interactions. It can help citizens to (a) report problems and (b) gather information related to civic issues for different locations and their neighborhoods. We report the results of a user study carried out to establish the effectiveness of our interface and draw a comparison with an existing platform. A detailed qualitative and quantitative analysis of the survey results shows a definite and statistically significant (p < 0.05) preference for our interface over the existing platform.},
	author = {Atreja, S. and Dumrewal, A. and Aggarwal, P. and Basu, A. and Mohapatra, P. and Dasgupta, G.B.},
	year = {2018},
	keywords = {Conversational agent, Citizen engagement, Clustering, CRF, Knowledge mining, Natural language, Social good, Topic modeling},
	pages = {617--628},
	notes = {011.pdf}
}

@inproceedings{gintner_improving_2018,
	title = {Improving reverse geocoding: {Localization} of blind pedestrians using conversational {UI}},
	volume = {2018-January},
	doi = {10.1109/CogInfoCom.2017.8268232},
	booktitle={8th IEEE International Conference on Cognitive Infocommunications},
	abstract = {Geolocation services suffer from low precision in urban environments mainly due to sky occlusion and signal reflections from buildings, which is the key disadvantage of current navigation assistive AIDS for blind pedestrians. We designed a method that improves reverse geocoding to the level, at which the system can differentiate sides of the street where the blind user is traveling. By implementing a conversational user interface to ask the user about important landmarks and in combination with coarse location and heading, we achieved a usable, accessible and acceptable solution. The number of address points candidates can be reduced by 50 \% to those on the corresponding side of the street. A qualitative field study with six visually impaired participants confirmed acceptance by the user group and accessibility and usability of the method.},
	author = {Gintner, V. and Balata, J. and Boksansky, J. and Mikovec, Z.},
	year = {2018},
	notes = {027.pdf}
}

@inproceedings{klopfenstein_code_2018,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Code {Hunting} {Games}: {A} {Mixed} {Reality} {Multiplayer} {Treasure} {Hunt} {Through} a {Conversational} {Interface}},
	volume = {10750 LNCS},
	booktitle={International Conference on Internet Science},
	abstract = {In this paper, we describe an online multi-player game that challenges players with abstract coding puzzles that are tied to a geographical location. The proposed system transposes the classical scheme of “treasure hunt” games into a mixed-reality game, where players must physically move in order to advance in the game, while at the same time interacting with a chatbot through an online messaging system. The implementation of the online game is described in detail and an overview of different deployments of the system is given, including a large-scale deployment during the European CodeWeek 2017. We discuss details of the proposed system, including lessons learned during the development and operation of the game. We also argue that mobile games like the one proposed can be successfully adopted for many different purposes, from entertainment to education.},
	author = {Klopfenstein, L.C. and Delpriori, S. and Paolini, B.D. and Bogliolo, A.},
	year = {2018},
	keywords = {Bots, Conversational UI, Instant messaging, Mixed reality, Mobile gaming},
	notes = {014.pdf}
}

@inproceedings{gupta2018,
	title = {Raiden11{@IECSIL}-{FIRE}-2018: {Named} entity recognition for {Indian} languages},
	volume = {2266},
	booktitle = {Forum for Information Retrieval Evaluation},
	abstract = {This paper presents our solution for the Named Entity Recognition (NER) task for the Information Extractor for Conversational Systems in Indian Languages challenge (IECSIL) [5] of the FIRE 2018 conference. A subset of the Information Extraction (IE) task, NER is a key to extract information and semantics of the text from unstructured data. The objective of NER is the identification and classification of every word or token in a document into predefined categories such as names of person, location, organization, etc. For this challenge the dataset provided by IECSIL [4] comprised of multilingual text of various Indian languages like Hindi, Tamil, Malayalam, Telugu, and Kannada. We mainly focus on the identification and classification of named entities belonging to nine categories like Name, Location, Datenum, etc. We tried linear models like Naive Bayes and SVM, and also a simple Neural Network to solve this problem. The best results are achieved by the simple neural network with an accuracy of 90.33\% for all languages combined. This indicates that different advanced neural networks could be possible solutions to further improve this accuracy.},
	author = {Gupta, A. and Ayyar, M. and Singh, A.K. and Shah, R.R.},
	year = {2018},
	keywords = {Information Extraction, Named Entity Recognition, Neural Networks, Word Embeddings},
	pages = {174--186},
	notes = {043.pdf}
}

@inproceedings{eiris-pereira_building_2018,
	title = {Building intelligent virtual agents as conversational partners in digital construction sites},
	volume = {2018-April},
	doi = {10.1061/9780784481264.020},
	booktitle={Construction Research Congress 2018},
	abstract = {Construction-related professionals are required to interact constantly on a variety of complex spatiotemporal occasions with several groups of people (e.g., architects, engineers, other construction professionals). The efficiency and effectiveness of the construction process strongly depends on the successful communication of information between all the functional groups involved in the different contexts of construction projects. However, lack of exposure to construction processes and construction professionals is widely observed in construction graduates. This lack of exposure results in deficient understanding of construction domain concepts in relation to real world problems. The use of intelligent virtual agents powered by building information modeling (BIM)-based virtual environments provides opportunities to incorporate conversational practices into classroom teaching. By employing these virtual interactions, a consequence-free environment that is controllable, representative, and repeatable can be achieved, with the benefit of providing constant real-time feedback to students. In such digital settings, students can observe spatiotemporal dependent occasions and communicate with other virtual professionals to obtain a better understanding of the construction events. In this paper, the technical workflow to create this virtual communication platform will be described in detail. This workflow includes (1) procedure of generating and authoring a virtual agent with its corresponding layers of information, (2) the creation process of the virtual environment that corresponds the conversational contexts using BIM-based technologies, and (3) the integration of the intelligent virtual agent and the digital site into a working platform. A case study is used to illustrate the conversational practice platform workflow for a high-risk caught-in or -between hazard scenario.},
	author = {Eiris-Pereira, R. and Gheisari, M.},
	year = {2018},
	pages = {200--209},
	notes = {010.pdf}
}

@inproceedings{thenmozhi2018,
	title = {{SSN}\_NLp{@IECSIL}-{FIRE}-2018: {Deep} learning approach to named entity recognition and relation extraction for conversational systems in {Indian} languages},
	volume = {2266},
	booktitle = {Forum for Information Retrieval Evaluation},
	abstract = {Named Entity Recognition (NER) focuses on the classification of proper nouns into the generic named entities (NE) such as person\_names, organizations, locations, currency and dates. NER has several applications like conversation systems, machine translation, automatic summarization and question answering. Relation Extraction (RE) is an information extraction process used to identify the relationship between NEs. RE is very important in applications like short answer grading, conversation systems, question answering and ontology learning. NER and RE in Indian languages are difficult tasks due to their agglutinative nature and rich morphological structure. Further, developing language independent framework that supports all Indian Languages is a challenging task. In this paper, we present a deep learning methodology for both NER and RE in five Indian languages namely Hindi, Kannada, Malayalam, Tamil and Telugu. We proposed a common approach that works for both NER and RE tasks. We have used neural machine translation architecture to implement our methodology for these tasks. Our approach was evaluated using the data set given by IECSIL@FIRE2018 shared task. We have evaluated on two sets of data for NER task and obtained the accuracies as 94.41\%, 95.23\%, 95.97\% and 96.02\% for the four variations on pre-evaluation test set and 95.9\%, 95.85\% and 95.05\% for the three runs on final-evaluation test set. Also, for RE task, we have obtained the accuracies as 56.19\%, 60.74\%, 60.7\%, 75.43\% and 79.11\% for our five variations on pre-evaluation test set and 79.44\%, 76.01\% and 61.11\% for Run 1, Run 2 and Run 3 respectively on final-evaluation test set.},
	author = {Thenmozhi, D. and Senthil Kumar, B. and Aravindan, C.},
	year = {2018},
	keywords = {Information Extraction, Deep Learning, Indian Languages, Named Entity Recognition (NER), Relation Extraction, Text mining},
	pages = {187--201},
	notes = {046.pdf}
}

@article{bartie_dialogue_2018,
	title = {A dialogue based mobile virtual assistant for tourists: {The} {SpaceBook} {Project}},
	volume = {67},
	doi = {10.1016/j.compenvurbsys.2017.09.010},
	abstract = {Ubiquitous mobile computing offers innovative approaches in the delivery of information that can facilitate free roaming of the city, informing and guiding the tourist as the city unfolds before them. However making frequent visual reference to mobile devices can be distracting, the user having to interact via a small screen thus disrupting the explorative experience. This research reports on an EU funded project, SpaceBook, that explored the utility of a hands-free, eyes-free virtual tour guide, that could answer questions through a spoken dialogue user interface and notify the user of interesting features in view while guiding the tourist to various destinations. Visibility modelling was carried out in real-time based on a LiDAR sourced digital surface model, fused with a variety of map and crowd sourced datasets (e.g. Ordnance Survey, OpenStreetMap, Flickr, Foursquare) to establish the most interesting landmarks visible from the user's location at any given moment. A number of variations of the SpaceBook system were trialled in Edinburgh (Scotland). The research highlighted the pleasure derived from this novel form of interaction and revealed the complexity of prioritising route guidance instruction alongside identification, description and embellishment of landmark information – there being a delicate balance between the level of information ‘pushed’ to the user, and the user's requests for further information. Among a number of challenges, were issues regarding the fidelity of spatial data and positioning information required for pedestrian based systems – the pedestrian having much greater freedom of movement than vehicles.},
	journal = {Computers, Environment and Urban Systems},
	author = {Bartie, P. and Mackaness, W. and Lemon, O. and Dalmas, T. and Janarthanam, S. and Hill, R.L. and Dickinson, A. and Liu, X.},
	year = {2018},
	keywords = {Location based service, Spoken Dialogue System, Viewshed, Virtual city guide},
	pages = {110--123},
	notes = {001.pdf}
}

@inproceedings{agarwal_remembering_2017,
	title = {Remembering what you said: {Semantic} personalized memory for personal digital assistants},
	doi = {10.1109/ICASSP.2017.7953275},
	booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
	abstract = {Personal digital assistants are designed to assist users in easy information retrieval or execute the tasks they are interested in. The conversational medium implies an additional level of intelligence but typically these systems do not support any reference to the user's past interactions. We propose a domain-agnostic approach that enables the system to address queries referring to the past by using an information retrieval approach to rank various entities for a given query. We also add semantic enrichment to the recall process by augmenting the entities with information from a knowledge graph and leverage that in the retrieval process. We mined user interactions for the Cortana digital assistant to extract queries with location and business entities and show that our technique can achieve an accuracy of 89.8\% for such recall queries.},
	author = {Agarwal, V. and Khan, O.Z. and Sarikaya, R.},
	year = {2017},
	keywords = {dialog management, information retrieval, Personal digital assistants, referring expressions, spoken language understanding},
	pages = {5835--5839},
	notes = {044.pdf}
}

@inproceedings{zhu_using_2017,
	title = {Using knowledge graph and search query click logs in statistical language model for speech recognition},
	volume = {2017-August},
	doi = {10.21437/Interspeech.2017-1790},
	booktitle={Proc. Interspeech 2017},
	abstract = {This paper demonstrates how Knowledge Graph (KG) and Search Query Click Logs (SQCL) can be leveraged in statistical language models to improve named entity recognition for online speech recognition systems. Due to the missing in the training data, some named entities may be recognized as other common words that have the similar pronunciation. KG and SQCL cover comprehensive and fresh named entities and queries that can be used to mitigate the wrong recognition. First, all the entities located in the same area in KG are clustered together, and the queries that contain the entity names are selected from SQCL as the training data of a geographical statistical language model for each entity cluster. These geographical language models make the unseen named entities less likely to occur during the model training, and can be dynamically switched according to the user location in the recognition phase. Second, if any named entities are identified in the previous utterances within a conversational dialog, the probability of the n-best word sequence paths that contain their related entities will be increased for the current utterance by utilizing the entity relationships from KG and SQCL. This way can leverage the long-Term contexts within the dialog. Experiments for the proposed approach on voice queries from a spoken dialog system yielded a 12.5\% relative perplexity reduction in the language model measurement, and a 1.1\% absolute word error rate reduction in the speech recognition measurement.},
	author = {Zhu, W.},
	year = {2017},
	keywords = {knowledge graph, named entity recognition, search query click log, speech recognition, statistical language model},
	pages = {2735--2738},
	notes = {048.pdf}
}

@article{garrido_smart_2017,
	title = {Smart tourist information points by combining agents, semantics and {AI} techniques},
	volume = {14},
	doi = {10.2298/CSIS150410029G},
	abstract = {The tourism sector in the province of Teruel (Aragon, Spain) is increasing rapidly. Although the number of domestic and foreign tourists is continuously growing, there are some tourist attractions spread over a wide geographical area, which are only visited by a few people at specific times of the year. Additionally, having human tourist guides everywhere and speaking different languages is unfeasible. An integrated solution based on smart and interactive Embodied Conversational Agents (ECAs) tourist guides combined with ontologies would overcome this problem. This paper presents a smart tourist information points approach which gathers tourism information about Teruel, structured according to a novel lightweight ontology built on OWL (Ontology Web Language), known as TITERIA (Touristic Information of TEruel for Intelligent Agents). Our proposal, which combines TITERIA with the Maxine platform, is capable of responding appropriately to the users thanks to its Artificial Intelligence Modeling Language (AIML) database and the AI techniques added to Maxine. Preliminary results indicate that our prototype is able to inform users about interesting topics, as well as to propose other related information, allowing them to acquire a complete information about any issue. Furthermore, users can directly talk with an artificial actor making communication much more natural and closer.},
	number = {1},
	journal = {Computer Science and Information Systems},
	author = {Garrido, P. and Barrachina, J. and Martinez, F.J. and Seron, F.J.},
	year = {2017},
	keywords = {Virtual human, Embodied conversational agent, Ontology, Tourism},
	pages = {1--23},
	notes = {045.pdf}
}

@inproceedings{signoretti_trip_2015,
	title = {Trip 4 {All}: {A} {Gamified} {App} to {Provide} a {New} {Way} to {Elderly} {People} to {Travel}},
	volume = {67},
	doi = {10.1016/j.procs.2015.09.274},
	booktitle={Proceedings of the 6th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-exclusion},
	abstract = {Older adults have much to gain from bringing technology into their daily lives. The extent to which this is possible strongly depends on careful design and accessible, easy to use products, developed using an elderly centered methodology. The senior tourism is a market in expansion and the old travelers need new and innovative technologies to help and support their trips. These technologies should contribute to a fun and safe experience, while promoting feelings of pleasure and self realization. In this paper we follow this design approach and put it to the test in developing the "Trip 4 All"(T4A), an application that works as a gamified virtual assistant to the elderly during a walking tourist visit. The gamified interaction with the visited environment intend to improve motivation to accomplish the visit and make the content absorption more fun and easier. The T4A works on georeferenced maps where the users' geoposition is a trigger to launch storytelling content and/or challenges based on the aspects of the visited site as such: geographical, art, religious, historic, cultural and human. The success in the challenges give the user prizes, new resources and abilities to try more complex challenges that brings more valuable prizes and so on. Furthermore, the proposed application intend to work as a companion that provides self confidence, support and social integration to elderly tourists.},
	author = {Signoretti, A. and Martins, A.I. and Almeida, N. and Vieira, D. and Rosa, A.F. and Costa, C.M.M. and Texeira, A.},
	year = {2015},
	keywords = {Active Aging, Elderly-centred design, Iterative development method, Mobile application evaluation},
	pages = {301--311},
	notes = {047.pdf}
}

@article{cai_modeling_2013,
	title = {Modeling and communicating the conceptual intent of geo-analytical tasks for human-{GIS} interaction},
	volume = {17},
	doi = {10.1111/tgis.12040},
	abstract = {One of the fundamental issues of geographical information science is to design GIS interfaces and functionalities in a way that is easy to understand, teach, and use. Unfortunately, current geographical information systems (including ArcGIS) remains very difficult to use as spatial analysis tools, because they organize and expose functionalities according to GIS data structures and processing algorithms. As a result, GIS interfaces are conceptually confusing, cognitively complex, and semantically disconnected from the way human reason about spatial analytical activities. In this article, we propose an approach that structures GIS analytical functions based on the notion of "analytical intent". We describe an experiment that replaces ArcGIS desktop interface with a conversational interface, to enable mixed-initiative user-system interactions at the level of analytical intentions. We initially focus on the subset of GIS functions that are relevant to "finding what's inside" as described by Mitchell, but the general principles apply to other types of spatial analysis. This work demonstrates the feasibility of delegating some spatial thinking tasks to computational agents, and also raises future research questions that are key to building a better theory of spatial thinking with GIS.},
	number = {3},
	journal = {Transactions in GIS},
	author = {Cai, G. and Yu, B. and Chen, D.},
	year = {2013},
	pages = {353--368},
	notes = {035.pdf}
}

@inproceedings{janarthanam_conversational_2012,
	title = {Conversational natural language interaction for place-related knowledge acquisition},
	volume = {881},
	booktitle={International Workshop on Place-related Knowledge Acquisition Research},
	abstract = {We focus on the problems of using Natural Language interaction to support pedestrians in their place-related knowledge acquisition. Our case study for this discussion is a smartphone-based Natural Language interface that allows users to acquire spatial and cultural knowledge of a city. The framework consists of a spoken dialogue-based information system and a smartphone client. The system is novel in combining geographic information system (GIS) modules such as a visibility engine with a question-answering (QA) system. Users can use the smart-phone client to engage in a variety of interleaved conversations such as navigating from A to B, using the QA functionality to learn more about points of interest (PoI) nearby, and searching for amenities and tourist attractions. This system explores a variety of research questions involving Natural Language interaction for acquisition of knowledge about space and place.},
	author = {Janarthanam, S. and Lemon, O. and Liu, X. and Bartie, P. and Mackaness, W. and Dalmas, T. and Goetze, J.},
	year = {2012},
	pages = {33--38},
	notes = {018.pdf}
}

@inproceedings{cuayahuitl_optimizing_2011,
	title = {Optimizing situated dialogue management in unknown environments},
	booktitle={Twelfth Annual Conference of the International Speech Communication Association},
	abstract = {We present a conversational learning agent that helps users navigate through complex and challenging spatial environments. The agent exhibits adaptive behaviour by learning spatiallyaware dialogue actions while the user carries out the navigation task. To this end, we use Hierarchical Reinforcement Learning with relational representations to efficiently optimize dialogue actions tightly-coupled with spatial ones, and Bayesian networks to model the user's beliefs of the navigation environment. Since these beliefs are continuously changing, we induce the agent's behaviour in real time. Experimental results, using simulation, are encouraging by showing efficient adaptation to the user's navigation knowledge, specifically to the generated route and the intermediate locations to negotiate with the user.},
	author = {Cuayáhuitl, H. and Dethlefs, N.},
	year = {2011},
	keywords = {Situated interaction, Bayesian networks, Hierarchical control, Reinforcement learning, Spoken dialogue systems},
	pages = {1009--1012},
	notes = {039.pdf}
}

@inproceedings{dobnik_human_2010,
	title = {Human evaluation of robot-generated spatial descriptions},
	volume = {620},
	booktitle = {Proceedings of the Workshop on Computational Models of Spatial Language Interpretation},
	abstract = {We describe a system where the semantics of spatial referential expressions have been automatically learned by finding mappings between symbolic natural language descriptions of the environment and non-symbolic representations from the sensory data of a mobile robot used for localisation and map building (SLAM). Although the success of learning can be measured by examining classifier performance on held-out data, this does not in itself guarantee that the descriptions generated will be natural and informative for a human observer. In this paper we describe the results of an evaluation of our embodied robotic system by human observers. Key words: spatial expressions, machine learning, mobile robots, embod- ied multi-modal conversational agents, evaluation.},
	author = {Dobnik, S. and Pulman, S.G.},
	year = {2010},
	keywords = {spatial expressions, machine learning, mobile robots, embodied multi-modal conversational agents, evaluation},
	pages = {25--32},
	notes = {025.pdf}
}

@inproceedings{sanchez-pi_multi-agent_2010,
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {Multi-agent system ({MAS}) applications in ambient intelligence ({AmI}) environments},
	booktitle={Trends in Practical Applications of Agents and Multiagent Systems},
	volume = {71},
	abstract = {Research in context-aware systems has been moving towards reusable and adaptable architectures for managing more advanced human-computer interfaces. Ambient. Intelligence (AmI) investigates computer-based services, which are ubiquitous and based on a variety of objects and devices. Their intelligent and intuitive interfaces act as mediators through which people can interact with the ambient environment. In this paper we present an agent-based architecture which supports the execution of agents in AmI environments. Two case studies are also presented, an airport information system and a railway information system, which uses spoken conversational agents to respond to the user’s requests using the contextual information that includes the location information of the user.},
	author = {Sánchez-Pi, N. and Mangina, E. and Carbó, J. and Molina, J.M.},
	year = {2010},
	pages={493--500},
	keywords = {Mobile context-aware systems, Multi-agent systems, Services oriented architectures},
	notes = {037.pdf}
}

@article{tsai_ask_nodate,
	title = {Ask {Diana}: {A} {Keyword}-{Based} {Chatbot} {System} for {Water}-{Related} {Disaster} {Management}},
	volume = {11},
	doi = {https://doi.org/10.3390/w11020234},
	abstract = {This research developed a keyword-based chatbot system, Ask Diana, for water-related disaster management. Disaster management has been considered difficult and tedious due to the complex characteristics of disaster-related data. To deal with this problem, this research developed a chatbot system with a water-related disaster database, a user intent mechanism, and an intuitive mobile-device-based user interface. With such a system, users are able to access important data or information they need for decision making by directly asking the proposed chatbot or operating the image-based menus. The system was validated through a usability test and a six-month field test. The results demonstrated that Ask Diana can help related personnel access disaster data intuitively and develop corresponding response strategies efficiently.},
	language = {English},
	year = {2019},
	keywords = {chatbot, disaster management, decision support, water-related disaster},
	number = {2},
	journal = {Water},
	author = {Tsai, Meng-Han and Chen, James Yichun and Kang, Shih-Chung},
	pages = {234},
	notes = {101.pdf}
}

@article{Ronzhin_2019, 
	title={Kadaster Knowledge Graph: Beyond the Fifth Star of Open Data}, 
	volume={10}, 
	abstract = {After more than a decade, the supply-driven approach to publishing public (open) data has resulted in an ever-growing number of data silos. Hundreds of thousands of datasets have been catalogued and can be accessed at data portals at different administrative levels. However, usually, users do not think in terms of datasets when they search for information. Instead, they are interested in information that is most likely scattered across several datasets. In the world of proprietary in-company data, organizations invest heavily in connecting data in knowledge graphs and/or store data in data lakes with the intention of having an integrated view of the data for analysis. With the rise of machine learning, it is a common belief that governments can improve their services, for example, by allowing citizens to get answers related to government information from virtual assistants like Alexa or Siri. To provide high-quality answers, these systems need to be fed with knowledge graphs. In this paper, we share our experience of constructing and using the first open government knowledge graph in the Netherlands. Based on the developed demonstrators, we elaborate on the value of having such a graph and demonstrate its use in the context of improved data browsing, multicriteria analysis for urban planning, and the development of location-aware chat bots.},
	url={http://dx.doi.org/10.3390/info10100310}, 
	DOI={10.3390/info10100310}, 
	number={10}, 
	journal={Information}, 
	publisher={MDPI AG}, 
	author={Ronzhin and Folmer and Maria and Brattinga and Beek and Lemmens and {van’t} Veer}, 
	year={2019}, 
	keywords = {linked data, knowledge graph, semantic enrichment, location-aware chat bots, governmental open data},
	month={Oct}, 
	pages={310},
	notes={049.pdf}	 
}

@article{marge_2019,
	author = {Marge, Matthew and Rudnicky, Alexander I.},
	title = {Miscommunication Detection and Recovery in Situated Human–Robot Dialogue},
	year = {2019},
	abstract = {Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot’s path planner and surroundings.},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {9},
	number = {1},
	url = {https://doi.org/10.1145/3237189},
	doi = {10.1145/3237189},
	journal = {ACM Transactions on Interactive Intelligent Systems},
	month = {feb},
	numpages = {40},
	keywords = {human–robot interaction, Human–robot communication, spoken-dialogue systems, language grounding, physically situated dialogue},
	notes = {050.pdf}
}


@inproceedings{KLOPFENSTEIN2018XMA,
	author = {Klopfenstein, L. and Delpriori, S. and Paolini, B. and Bogliolo, A.},
	title = {X MARKS THE BOT: ONLINE CODING-BASED TREASURE HUNT GAMES FOR CODE LITERACY},
	series = {12th International Technology, Education and Development Conference},
	booktitle = {INTED2018 Proceedings},
	isbn = {978-84-697-9480-7},
	issn = {2340-1079},
	doi = {10.21125/inted.2018.0951},
	url = {http://dx.doi.org/10.21125/inted.2018.0951},
	publisher = {IATED},
	location = {Valencia, Spain},
	month = {5-7 March, 2018},
	year = {2018},
	keywords = {coding, code literacy, online games, chatbot, treasure hunt},
	abstract = {Coding games, both computer-based and "unplugged" ones, have been increasingly used over the past years to promote code literacy and to bring basic programming concepts to the larger public, in particular to children in preschool , elementary, and middle school ages. Chatbots on instant messaging (IM) platforms provide a modern and friction-free interface that allow software developers to instantly connect with a wide audience, making use of familiar conversational interface patterns. "Code Hunting Games" is a chatbot-based multiplayer game, which engages multiple teams in a "treasure hunt" guided by a Telegram bot. On their track to the treasure, teams are challenged with a sequence of coding puzzles that must be solved using the conversational interface. At the same time, teams must move through physical space in order to reach geographical locations marked by a code, as indicated by the bot. In this paper we present the implementation of the game and describe several large-scale deployments, including one Europe-wide game session during Code Week 2017 with more than 160 competing teams. The system is freely available online for custom game sessions. We discuss the game's impact, in terms of engagement for children in particular, lessons learned during development and deployment, and future work.},
	pages = {4867--4873},
	notes = {053.pdf}
}

@article{Doumanis_Smith_2015, 
	title={A Framework for Research in Gamified Mobile Guide Applications using Embodied Conversational Agents (ECAs)}, 
	volume={2}, 
	url={https://journal.seriousgamessociety.org/index.php/IJSG/article/view/79}, 
	DOI={10.17083/ijsg.v2i3.79}, 
	abstract={Mobile Guides are mobile applications that provide players with local and location-based services (LBS), such as navigation assistance, where and when they need them most. Advances in mobile technologies in recent years have enabled the gamification of these applications, opening up new opportunities to transfer education and culture through game play. However, adding traditional game elements such as PBLs (points, badges, and leaderboards) alone cannot ensure that the intended learning outcomes will be met, as the player’s cognitive resources are shared between the application and the surrounding environment. This distribution of resources prevents players from easily immersing themselves into the educational scenario. Adding artificial conversational characters (ECAs) that simulate the social norms found in real-life human-to-human guide scenarios has the potential to address this problem and improve the player’s experience and learning of cultural narratives. Although significant progress has been made towards creating game-like mobile guides with ECAs, there is still a lack of a unified framework that enables researchers and practitioners to investigate the potential effects of such applications to players and how to approach the concepts of player experience, cognitive accessibility and usability in this context. This paper presents a theoretically-well supported research framework consisted of four key components: differences in players, different features of the gamified task, aspects of how the ECA looks, sound or behaves and different mobile environments. Furthermore, it provides based on this framework a working definition of what player experience, cognitive accessibility and usability are in the context of game-like mobile guide applications. Finally, a synthesis of the results of six empirical studies conducted within this research framework is discussed and a series of design guidelines for the effective gamification of mobile guide applications using ECAs are presented. Results show that an ECA can positively affect the quality of the player’s experience, but it did not elicit better player retention of cultural narratives and navigation of routes.}, 
	number={3}, 
	journal={International Journal of Serious Games}, 
	author={Doumanis, Ioannis and Smith, Serengul}, 
	year={2015}, 
	month={Sep.},
	keywords = {Gamification, Mobile Guide Applications (MGA), Embodied Conversational Agents (ECAs), Player experience, Cognitive Accessibility, Usability},
	notes = {054.pdf}
}

@article{antrobus_large_burnett_hare_2019, 
	title={Enhancing Environmental Engagement with Natural Language Interfaces for In-Vehicle Navigation Systems}, 
	volume={72}, 
	DOI={10.1017/S037346331800108X}, 
	number={3}, 
	journal={Journal of Navigation}, 
	publisher={Cambridge University Press}, 
	author={Antrobus, Vicki and Large, David and Burnett, Gary and Hare, Chrisminder}, 
	year={2019}, 
	abstract={Four on-road studies were conducted in the Clifton area of Nottingham, UK, aiming to explore the relationships between driver workload and environmental engagement associated with ‘active’ and ‘passive’ navigation systems. In a between-subjects design, a total of 61 experienced drivers completed two experimental drives comprising the same three routes (with overlapping sections), staged one week apart. Drivers were provided with the navigational support of a commercially-available navigation device (‘satnav’), an informed passenger (a stranger with expert route knowledge), a collaborative passenger (an individual with whom they had a close, personal relationship) or a novel interface employing a conversational natural language ‘NAV-NLI’ (Navigation Natural Language Interface). The NAV-NLI was created by curating linguistic intercourse extracted from the earlier conditions and delivering this using a ‘Wizard-of-Oz’ technique. This term describes a research experiment in which subjects interact with a computer system that they believe to be autonomous, but which is actually being operated or partially operated by an unseen human being. The different navigational methods were notable for their varying interactivity and the preponderance of environmental landmark information within route directions. Participants experienced the same guidance on each of the two drives to explore changes in reported and observed behaviour. Results show that participants who were more active in the navigation task (collaborative passenger or NAV-NLI) demonstrated enhanced environmental engagement (landmark recognition, route-learning and survey knowledge) allowing them to reconstruct the route more accurately post-drive, compared to drivers using more passive forms of navigational support (SatNav or informed passenger). Workload measures (the Tactile Detection Task (TDT) and the National Aeronautical and Space Administration Task Load Index (NASA-TLX)) indicated no differences between conditions, although SatNav users and collaborative passenger drivers reported lower workload during their second drive. The research demonstrates clear benefits and potential for a navigation system employing two-way conversational language to deliver instructions. This could help support a long-term perspective in the development of spatial knowledge, enabling drivers to become less reliant on the technology and begin to re-establish associations between viewing an environmental feature and the related navigational manoeuvre.},
	keywords={Natural Language, Navigation, Workload, Landmarks, Cognitive Map},
	pages={513--527}, 
	notes={056.pdf}
}


@article{ABATE2011415,
	title = {An interactive virtual guide for the AR based visit of archaeological sites},
	journal = {Journal of Visual Languages & Computing},
	volume = {22},
	number = {6},
	pages = {415--425},
	year = {2011},
	issn = {1045-926X},
	doi = {10.1016/j.jvlc.2011.02.005},
	url = {https://doi.org/10.1016/j.jvlc.2011.02.005},
	author = {Andrea F. Abate and Giovanni Acampora and Stefano Ricciardi},
	keywords = {Human–avatar interaction, Augmented reality, Virtual characters in real environments, Behavioural animation},
	abstract = {One of the most interesting research lines about avatars is the design and the implementation of a synthetic behaviour able to drive avatar's actions according to an adaptive interaction paradigm. This aspect, indeed, is of fundamental importance to many advanced applications involving avatars and humans. This study presents a novel framework exploiting augmented reality to visualize a synthetic 3D virtual guide inside an exhibit or a public gallery, to assist visitors wearing a Head Mounted Display during their visit and providing them with both visual and informative enhancements not available in a standard tour. The Human–avatar interaction is approached through a model based on timed automata to address the conversational issues and to improve the quality of interaction by means of an effective synchronization. A usability study conducted on an application of this research to the “avatar assisted tour” of a roman villa, confirms the efficacy of the approach.},
	notes = {057.pdf}
}

@article{SANTOS2016194,
	title = {An IoT-based mobile gateway for intelligent personal assistants on mobile health environments},
	journal = {Journal of Network and Computer Applications},
	volume = {71},
	pages = {194--204},
	year = {2016},
	issn = {1084-8045},
	doi = {10.1016/j.jnca.2016.03.014},
	url = {https://doi.org/10.1016/j.jnca.2016.03.014},
	author = {João Santos and Joel J.P.C. Rodrigues and Bruno M.C. Silva and João Casal and Kashif Saleem and Victor Denisov},
	keywords = {Intelligent personal assistant, Mobile health, Body sensor network, Internet of things, Mobile gateway, Smart object},
	abstract = {The evolution of mobile devices has triggered the appearance of intelligent personal assistants (IPAs). IPAs are software agents used to support users to fulfill several daily actions. They are supposed to be intelligent in such a way that allows them to give their owners advices about many different subjects. To do so, IPAs must learn about their user behavior and routines. With the current state of the art technologies, scenarios of ubiquitous communication can be created. One of the potential enablers for those scenarios is the Internet of Things (IoT) paradigm where machines with decision support systems interact and communicate among them. In an IoT environment, IPAs can interact with other smart objects in order to gain new knowledge and awareness about their users. This paper proposes a novel IoT-based mobile gateway solution for mobile health (m-Health) scenarios. This gateway autonomously collects information about the user/patient location, heart rate, and possible fall detection. Moreover, it forwards the collected information to a caretaker IPA, in real time, that will manage a set of actions and alarms appropriately. The algorithms used for each mobile gateway service, and the scenarios where the mobile gateway acts as a communication channel or a smart object are also addressed on this paper.},
	notes = {058.pdf}
}