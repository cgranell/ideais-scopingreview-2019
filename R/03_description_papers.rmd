---
title: 'Text analysis of the eligible papers included in the scoping review'
author: "Carlos Granell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
  word_document:
    toc: yes
urlcolor: blue
---


```{r setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE)
```

This document is an exploratory analysis of the eligible papers, a collaborative research activity as part of the [IDEAIS project](https://sites.google.com/view/redideais).


## Required libraries

```{r load_libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(tidytext)
library(stringr)
library(kableExtra)
library(wordcloud)

library(quanteda)

# for deterministic cloud rendering
set.seed(nchar("IDEAIS - scoping review paper"))

```

## Data 

```{r data_source, echo=FALSE}
file_name <- "papers2010_2019"

papers <- readRDS(file = here("data", paste0(file_name, ".rda")))

papers <- 
  papers %>%
  mutate(type = ifelse(type=="InProceedings", "Conference paper", "Journal paper"))

n_papers <- nrow(papers)
```

In total, `r n_papers` were eligible (N=`r n_papers`). Some of the eligibible papers may not be openly available due to access restriction imposed by publishers. Therefore, the content of the papers is not provided in this repository. We indeed include the bibliographic information of each paper in the file `r file_name`, in the directory `data`.


## Table 1. Conferences and journals

Table 1 (Conferences and journals) was manualy created based on the tables below. The first table shows the conferences/journals per year. The second table shows the percentage distribution of conference papers and journal articles. 


```{r tbl1_years, echo=FALSE}
papers %>%
  mutate(source = ifelse(!is.na(journal), journal, booktitle)) %>%
  group_by(type, source, year) %>%
  summarise(n = n()) %>%
  arrange(type, year, source) %>%
  select(`Type`=type, 
         `Year`= year, 
         `Source` = source,
         `#`=n) %>%
  knitr::kable(format="html", escape = T, booktabs = TRUE, width=600) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  collapse_rows(1:2, valign = "top")

```


```{r tbl1_distribution, echo=FALSE}

papers %>%
  group_by(type) %>% 
  summarise(n = n()) %>%
  mutate(proportion  = n / n_papers, 
         proportion_lbl = paste0(round(proportion*100,0), "%")) %>%
  select(`Type of document` = type,
         `N` = n,
         `%`= proportion_lbl) %>%
  knitr::kable(format="html", escape = T, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

## Figure 2. Temporal distribution of eligibible papers 

```{r fig2, echo=FALSE}

totals_per_year <-
  papers %>%
  group_by(year) %>%
  summarise(total = n()) %>%
  complete(year = 2010:2020, fill = list(total = 0))


papers %>%
  group_by(type, year) %>% 
  summarise(n = n()) %>%
  complete(type, year = 2010:2020, fill = list(n = 0)) %>%
  left_join(totals_per_year, by="year") %>%
  ggplot(aes(x=year, y=n)) +
  geom_area(aes(fill=type), alpha=0.3, position = position_dodge(width = 0)) +
  geom_line(aes(color=type), size=1) +
  geom_point(aes(color=type), size=1.5) +
  geom_line(aes(y=total), color="black", linetype ="dashed",  size=0.5) +
  labs(#title="Temporal distribution of eligible papers", 
    x="Year", 
    y="Number of papers", 
    fill = "Publication type") +
    #caption="Source: authors") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  scale_colour_manual(values=c("#999999", "#E69F00")) +
  scale_y_continuous(breaks=seq(0,15,by=1)) +
  scale_x_continuous(breaks=seq(2010,2020,by=1)) +
  guides(color = FALSE) +
  theme(legend.position = "bottom") +
  theme_minimal() +
  theme(panel.grid.minor = element_blank()) +
  theme(panel.background = element_blank())
  # theme(panel.grid.major.x = element_blank())


ggsave(here("figs", "fig02.png"))

```


## Terms frequency analysis 


As part of the text analysis of papers, including wordcloud and terms frequency analysis,
we read the full list of abstracts from the `papers` and process them to create a [tidy](https://www.jstatsoft.org/article/view/v059i10) data structure without
[stop words](https://en.wikipedia.org/wiki/Stop_words). The stop words include specific words, such as `paper` and `authors`, which are included in many abstract, abbreviations such as `e.g.`, and terms particular to scientific articles, such as `summary`. Also all numeric literas are removed from the word list. Reference book to text mining in tidy format: [Text Mining with R](https://www.tidytextmining.com/)


```{r stopwords_abstract, echo=FALSE}
tidy_abstracts <- 
  papers %>%
  select(id, year, abstract) %>%
  arrange(id)
  
# create a table of all words
papers_words <- 
  tidy_abstracts %>%
  select(id, year, abstract) %>%
  unnest_tokens(word, abstract)

my_stop_words <- tibble(
  word = c(
    "et",
    "al",
    "fig",
    "e.g.",
    "i.e.",
    "eu",
    "http",
    "ing",
    "pp",
    "figure",
    "based",
    "â",
    "background", # used to structure an abstract
    "objective",
    "methods",
    "results",
    "conclusions",
    "authors",
    "paper",
    "research",
    "related",
    "proposed",
    "support",
    "approach"
    ),
  lexicon = "IDEAIS")

all_stop_words <- stop_words %>%
  bind_rows(my_stop_words)

# Get rid of numeric values (as words) from abstracts
suppressWarnings({
  no_numbers <- papers_words %>%
    filter(is.na(as.numeric(word)))
})


# Get list of words from abstracts without stopwords 
non_stop_words <- no_numbers %>%
  anti_join(all_stop_words, by = "word")
          

```

```{r calculate_stopword_stats, echo=FALSE}
total_words = nrow(papers_words)
after_cleanup = nrow(non_stop_words)
```

About `r round(after_cleanup/total_words * 100)` % (`r after_cleanup`) of the total words (`r total_words`) in all abstracts are considered non stop words.

_How many non-stop words does each abstract have?_

```{r non_stopwords_abstract, echo=FALSE}
non_stop_words_per_abstract <- 
  non_stop_words %>%
  group_by(id, year) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words))


non_stop_words_per_abstract %>%
  rename(`# non-stop words` = num_words) %>%
  knitr::kable(caption = "Abstracts ordered by number of words after removal of stopwords.",
               format="html", escape = T, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

_How many non-stop words does each year have?_


```{r non_stopwords_year, echo=FALSE}

non_stop_words_per_year <- 
  non_stop_words_per_abstract %>% 
  group_by(year) %>%
  summarise(num_words_year = sum(num_words)) %>%
  arrange(desc(num_words_year))

non_stop_words_per_year %>%
  rename(`# non-stop words` = num_words_year) %>%
  knitr::kable(caption = "Total number of words after removal of stopwords per year.",
               format="html", escape = T, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```



## Top wordstems and wordstem clouds 

```{r params, include=FALSE}
# chosen manually
minimum_occurence <- 99
max_words <- 100
```

The following table shows the number of occurrences of the `r max_words` most frequent wordstems in all abstracts.

```{r top_wordstem, echo=FALSE}

wordstems <- non_stop_words %>%
  mutate(wordstem = quanteda::char_wordstem(non_stop_words$word))


countYearsUsingWordstem <- function(the_word) {
  sapply(the_word, function(w) {
    wordstems %>%
      filter(wordstem == w) %>%
      group_by(year) %>%
      count() %>%
      nrow
  })
}

countPapersUsingWord <- function(the_word) {
  sapply(the_word, function(w) {
    non_stop_words %>%
      filter(word == w) %>%
      group_by(id) %>%
      count %>%
      nrow
  })
}



# top words
no_top_words <- 40  # Arbitrary  
top_words <- non_stop_words %>%
  group_by(word) %>%
  tally %>%
  arrange(desc(n)) %>%
  head(no_top_words) %>%
  mutate(n_abstracts = countPapersUsingWord(word))

top_words %>%
  select(Palabra = word,
         Repeticiones = n,
         `# resúmenes` = n_abstracts) %>%
  arrange(desc(Repeticiones), desc(`# resúmenes`)) %>%
  knitr::kable(caption = paste0("Ranking de los ", no_top_words," términos más repetidos\n (si igual # repeticiones, ordenados por mayor # de resumenes)"),
               format="html", escape = T, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


```{r echo=FALSE}
set.seed(1)

minimum_occurence <- 5

cloud_words <- non_stop_words %>%
  group_by(word) %>%
  tally %>%
  filter(n >= minimum_occurence) %>% 
  arrange(desc(n))
```

_Word cloud of `r nrow(cloud_words)` top words in abstracts_ 


```{r fig_cloud_words_abstract, echo=FALSE}

if (nrow(cloud_words) > 0) {  
  png(here("figs", "fig03_cloud_words_abstract.png"), width=1280,height=800)
  wordcloud(cloud_words$word, cloud_words$n, 
            scale=c(10,.5),
            max.words=Inf, 
            random.order=FALSE, 
            rot.per=.15, 
            colors=brewer.pal(8,"Dark2"))
  dev.off()
  
} else {
  warning("No input data for wordcloud.")
}
```

```{r table_cloud_words_abstract, echo=FALSE }
set.seed(1)

cloud_words %>%
  slice(1:15) %>%
  select(Word = word,
         Freq = n) %>%
  arrange(desc(Freq)) %>%
  knitr::kable(format="html", escape = T, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))


```



_Word cloud of keywords_


```{r echo=FALSE}

keywords <-
  select(papers, id, keyword=keywords) %>%
  separate_rows(keyword, sep=",") %>%
  drop_na(keyword) %>%
  mutate(keyword = stringr::str_to_lower(stringr::str_trim(keyword))) 

no_top_keywords <- nrow(cloud_words) # Same number as the top words in abstract 
no_top_keywords <- 40

countPapersUsingKeyword <- function(the_word) {
  sapply(the_word, function(w) {
    keywords %>%
      filter(keyword == w) %>%
      group_by(id) %>%
      count %>%
      nrow
  })
}


cloud_keywords <- keywords %>%
  group_by(keyword) %>%
  summarise(n = n())%>%
  arrange(desc(n)) %>%
  head(no_top_keywords) %>%
  mutate(n_papers = countPapersUsingKeyword(keyword),
         n = n * 10) %>%
  arrange(desc(n), desc(n_papers))

```


```{r fig_cloud_keywords, echo=FALSE}
if (nrow(cloud_keywords) > 0) {  
    png(here("figs", "fig04_cloud_keywords.png"))#, width=1280,height=800)
    wordcloud(cloud_keywords$keyword, cloud_keywords$n, 
              scale=c(5,.2),
              max.words=Inf, 
              fixed.asp = TRUE,
              random.order=FALSE, 
              rot.per=0.25, 
              colors=brewer.pal(8,"Dark2"))
    #TODO: DOES NOT PRODUCE FIGURE!!!!
    dev.off()
  
  # plot.new()
  # wordcloud(cloud_keywords$keyword, cloud_keywords$n,
  #           max.words = Inf,
  #           random.order = FALSE,
  #           scale = c(1.5,.2),
  #           fixed.asp = TRUE,
  #           rot.per = 0,
  #           color = brewer.pal(8,"Dark2"))

} else {
  warning("No input data for wordcloud.")
}
```
```{r table_cloud_keywords, echo=FALSE}
##TODO: freq table for cloud keywords
```


## Runtime environment description


```{r session_info, echo=FALSE}
devtools::session_info(include_base = TRUE)
```